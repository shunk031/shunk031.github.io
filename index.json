
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Career LINE ãƒ¤ãƒ•ãƒ¼æ ªå¼ä¼šç¤¾ãƒªã‚µãƒ¼ãƒã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãƒ»æ³•æ”¿å¤§å­¦å¤§å­¦é™¢ç‰¹ä»»ç ”ç©¶å“¡ã€‚2023 å¹´ 3 æœˆã«æ³•æ”¿å¤§å­¦å¤§å­¦é™¢ç†å·¥å­¦ç ”ç©¶ç§‘ã‚’ä¿®äº†ã€‚åšå£«ï¼ˆå·¥å­¦ï¼‰ã€‚æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ï¼ˆDC2ï¼‰ã‚’çµŒã¦ç¾è·ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãƒ“ã‚¸ãƒ§ãƒ³ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã‚’å§‹ã‚ã€ãã®èåˆé ˜åŸŸã§ã‚ã‚‹ Vision\u0026amp;Language åˆ†é‡ã«ã¦ç ”ç©¶ã«å¾“äº‹ã€‚ç¾åœ¨ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦é­…åŠ›çš„ãªç”»åƒã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ã®ä½œæˆã‚’æ”¯æ´ã™ã‚‹ã‚ˆã†ãªæœ€å…ˆç«¯æŠ€è¡“ã®ç ”ç©¶é–‹ç™ºã«æºã‚ã‚‹ã€‚è‘—æ›¸ã«ã€ŒPython ã§å­¦ã¶ç”»åƒç”Ÿæˆã€ï¼ˆå˜è‘—ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¹ç¤¾ï¼‰ãŒã‚ã‚‹ã€‚å€‹äººãƒšãƒ¼ã‚¸: https://shunk031.me\nSee also the English user page, Shunsuke Kitada.\n","date":1772982000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1770812977,"objectID":"e586e4dac59a7326286aad9bd11c7350","permalink":"https://shunk031.me/authors/%E5%8C%97%E7%94%B0-%E4%BF%8A%E8%BC%94/","publishdate":"2026-01-09T00:00:00+09:00","relpermalink":"/authors/%E5%8C%97%E7%94%B0-%E4%BF%8A%E8%BC%94/","section":"authors","summary":"Career LINE ãƒ¤ãƒ•ãƒ¼æ ªå¼ä¼šç¤¾ãƒªã‚µãƒ¼ãƒã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãƒ»æ³•æ”¿å¤§å­¦å¤§å­¦é™¢ç‰¹ä»»ç ”ç©¶å“¡ã€‚2023 å¹´ 3 æœˆã«æ³•æ”¿å¤§å­¦å¤§å­¦é™¢ç†å·¥å­¦ç ”ç©¶ç§‘ã‚’ä¿®äº†ã€‚åšå£«ï¼ˆå·¥å­¦ï¼‰ã€‚æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ï¼ˆDC2ï¼‰ã‚’çµŒã¦ç¾è·ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãƒ“ã‚¸ãƒ§ãƒ³ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã‚’å§‹ã‚ã€ãã®èåˆé ˜åŸŸã§ã‚ã‚‹ Vision\u0026Language åˆ†é‡ã«ã¦ç ”ç©¶ã«å¾“äº‹ã€‚ç¾åœ¨ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦é­…åŠ›çš„ãªç”»åƒã‚„ãƒ‡ã‚¶ã‚¤ãƒ³ã®ä½œæˆã‚’æ”¯æ´ã™ã‚‹ã‚ˆã†ãªæœ€å…ˆç«¯æŠ€è¡“ã®ç ”ç©¶é–‹ç™ºã«æºã‚ã‚‹ã€‚è‘—æ›¸ã«ã€ŒPython ã§å­¦ã¶ç”»åƒç”Ÿæˆã€ï¼ˆå˜è‘—ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ã‚¹ç¤¾ï¼‰ãŒã‚ã‚‹ã€‚å€‹äººãƒšãƒ¼ã‚¸: https://shunk031.me\n","tags":null,"title":"åŒ—ç”° ä¿Šè¼”","type":"authors"},{"authors":null,"categories":null,"content":"Shunsuke Kitada ï¼ˆåŒ—ç”° ä¿Šè¼” in Japanese) is a Research Scientist at Image and Video AI Dept., LY Corp. His research interest is now on computational advertising with a focus on automatic geneartion/evaluation and assistive technology for multi-modal ad creatives.\nPreviously, he got his Ph.D. in 2023 at major in applied informatics, graduate school of science and engineering, Hosei University under the supervision of Prof. Hitoshi Iyatomi. His project was to improve prediction performance and model interpretability through attention mechanisms from basic and applied research perspectives.\nHis personality traits can be summarized as follows:\nâ¤ï¸ Love research and development. He is enjoying research life, and he is currently focusing on three research topics simultaneously, e.g., natural language processing, medical image-based computer vision, computational advertising. ğŸ“ Every day read and implement the SoTA models. He has released many re-implementations of models using mainly Chainer and PyTorch. Therefore, based on state-of-the-art cases, he can advise on deep learning-based product design. ğŸ˜„ High technical communicativity. Summarize what he made and what he studied, and spread such information. This shows that he can input and output regularly. The resume is available in PDF .\nBackground image borrowed from OpenAI ChatGPT illustration by Ruby Chen.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1770812977,"objectID":"202102d1d71aac401dd37b0b4fc8eb7c","permalink":"https://shunk031.me/authors/shunsuke-kitada/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shunsuke-kitada/","section":"authors","summary":"Shunsuke Kitada ï¼ˆåŒ—ç”° ä¿Šè¼” in Japanese) is a Research Scientist at Image and Video AI Dept., LY Corp. His research interest is now on computational advertising with a focus on automatic geneartion/evaluation and assistive technology for multi-modal ad creatives.\n","tags":null,"title":"Shunsuke Kitada, Ph.D.","type":"authors"},{"authors":["ç«¹ä¸‹ ç†æ–—","å·ç”° æ‹“æœ—","å¤§æ©‹ å·§","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI Agent"],"content":"","date":1772982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c2ccebd7f193c37e8439326080c198dd","permalink":"https://shunk031.me/publication/takeshita2026nlp/","publishdate":"2026-01-09T00:00:00+09:00","relpermalink":"/publication/takeshita2026nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 32 å›å¹´æ¬¡å¤§ä¼šï¼Œ2026.","tags":["Domestic Conference","Non-refereed","AI Agent","Computer Use","ANLP","ANLP2026"],"title":"Compressed-a11y: è¦–è¦šçš„æ–‡è„ˆã®å†æ§‹æˆã¨å†—é•·æ€§å‰Šæ¸›ã«ã‚ˆã‚‹ GUI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¦³æ¸¬ã®åŠ¹ç‡åŒ–","type":"publication"},{"authors":["èŠè°· å¹¹","åŒ—ç”° ä¿Šè¼”","åŸ è¡"],"categories":["Natural Language Processing","Gemma Scope","Mechanistic Interpretability"],"content":"","date":1772982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"fae60a1a721d5654ec6e0eab9f1dc664","permalink":"https://shunk031.me/publication/kikuya2026nlp/","publishdate":"2026-01-09T00:00:00+09:00","relpermalink":"/publication/kikuya2026nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 32 å›å¹´æ¬¡å¤§ä¼šï¼Œ2026.","tags":["Domestic Conference","Non-refereed","Interpretability","Mechanistic Interpretability","ANLP","ANLP2026"],"title":"MLP ã®é‡ã¿ã‚’åæ˜ ã—ãŸ Sparse Autoencoder ã®åˆæœŸåŒ–æ‰‹æ³•ã®ææ¡ˆ","type":"publication"},{"authors":["å·ç”° æ‹“æœ—","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1772982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"481a77280b2561199fc107f4b665d0ab","permalink":"https://shunk031.me/publication/kawada2026nlp/","publishdate":"2026-01-09T00:00:00+09:00","relpermalink":"/publication/kawada2026nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 32 å›å¹´æ¬¡å¤§ä¼šï¼Œ2026.","tags":["Domestic Conference","Non-refereed","AI for Science","Natural Language Processing","Computer Vision","Vision \u0026 Language","ANLP","ANLP2026"],"title":"SciGA-Vec: å­¦è¡“è«–æ–‡ã«ãŠã‘ã‚‹ãƒ™ã‚¯ã‚¿ç”»åƒå½¢å¼ã® Graphical Abstract ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","åŸ è¡"],"categories":["Natural Language Processing","Large Language Models","Mechanistic Interpretability"],"content":"","date":1772982000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a55b6dad3a74c35aee26655dba9bf98b","permalink":"https://shunk031.me/publication/kitada2026nlp/","publishdate":"2026-01-09T00:00:00+09:00","relpermalink":"/publication/kitada2026nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 32 å›å¹´æ¬¡å¤§ä¼šï¼Œ2026.","tags":["Domestic Conference","Non-refereed","Interpretability","Mechanistic Interpretability","ANLP","ANLP2026"],"title":"ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã¯æ—¥æœ¬èªLLMã‚’å …ç‰¢ã«åˆ¶å¾¡ã§ãã‚‹ã‹ï¼Ÿ","type":"publication"},{"authors":[],"categories":["tech"],"content":"As your number of repositories grows through git clone, for both work and personal projects, it becomes harder to remember where you cloned things and which directory youâ€™re currently working in. You end up wasting time on cd and shell completion. On top of that, once you start creating branches for development or checking out other branches to review Pull Requests, the overhead of switching branches and managing stashes just keeps increasing. I often mess up branch switches, run into conflicts, or almost lose my changes.\nThese problems become even more visible once you start using coding agents like Claude Code or Codex. The more tasks you run in parallel, the more likely things are to collide if you only have a single working directory. And of course, you want to unleash your coding agents as much as possible ğŸ˜„.\nIn this article, Iâ€™ll show how combining the following three tools can significantly improve your daily development workflow. And yesâ€”please do use coding agents.\nUse ghq to standardize where repositories are cloned (so things donâ€™t fall apart as they grow) Use gwq to make git worktree operations painless (parallel work becomes the default) Use fzf to minimize directory hopping (list â†’ fuzzy search â†’ instant jump) Table of Contents Why git worktree Works So Well with Coding Agents The Tooling Stack Used in This Article ghq: Standardize Where You Clone Repositories fzf: Turn Any List into a â€œSelectable UIâ€ gwq: Manage Worktrees â€œthe ghq Wayâ€ Unifying clone and worktree Under the Same Root Setup: Point Both ghq and gwq to ~/ghq Creating a â€œJumpâ€ Command: ghq + fzf Summary: Centralize, Select, and Run in Parallel Why git worktree Works So Well with Coding Agents git worktree allows you to have multiple working directories (worktrees) for a single repository. The official Git documentation clearly explains the distinction between the main worktree and linked worktrees, and recommends cleaning up unused linked worktrees with git worktree remove. 1\nThe reason this works so well with coding agents is simple: your workspaces are isolated.\nTask A progresses in worktree A Task B progresses in worktree B Even if you run agents for both, theyâ€™re not fighting over the same repository Anthropicâ€™s best practices also introduce workflows where multiple Claude sessions are run simultaneously using worktrees. 2\nThe Tooling Stack Used in This Article ghq: Standardize Where You Clone Repositories ghq is a CLI tool for organizing cloned repositories. Its philosophy is to place them under a common root using a structure like host/owner/repo.\nWith the ghq list command, you can see all repositories youâ€™ve cloned locally. When combined with fzf (covered later), this becomes a powerful UI for jumping between repositories. The ghq list --full-path command outputs full paths and is also featured in the official ghq handbook as a common pattern to pair with fzf. 3\nfzf: Turn Any List into a â€œSelectable UIâ€ fzf is a fast, interactive fuzzy finder that runs in the terminal. It takes input from stdin, filters it in real time, and lets you select items interactively.\ngwq: Manage Worktrees â€œthe ghq Wayâ€ gwq is a CLI tool for efficiently managing git worktrees. Its README describes it as:\nâ€œJust like ghq manages clones, gwq manages worktrees.â€\nItâ€™s designed around fuzzy-finder-driven workflows, making it easy to create, switch, and delete worktrees.\nUnifying clone and worktree Under the Same Root The key idea of this article is:\nBy placing both original repositories and git worktree directories under the same root (e.g. ~/ghq), you can target a single location with fzf, making navigation lightning-fast.\nBelow is a concrete setup.\nSetup: Point Both ghq and gwq to ~/ghq ghq: Fix the Root to ~/ghq Configure ghq to use ~/ghq as its root. ghq reads this from git config.\n[ghq] root = ~/ghq With this setup, running ghq get github.com/owner/repo results in the following structure:\n~/ghq/ github.com/ owner/ repo/ gwq: Place Worktrees Under ~/ghq gwq is configured via ~/.config/gwq/config.toml, where you can set worktree.basedir and naming.template.\n[naming] template = \u0026#39;{{.Host}}/{{.Owner}}/{{.Repository}}={{.Branch}}\u0026#39; [worktree] basedir = \u0026#39;~/ghq\u0026#39; gwq supports template variables such as Host, Owner, Repository, and Branch, and even allows branch name sanitization. This is clearly documented in its README.\nI personally use the format:\n\u0026#39;{{.Host}}/{{.Owner}}/{{.Repository}}={{.Branch}}\u0026#39; If you instead follow the gwq default and separate directories by {{.Branch}}, worktrees end up nested inside the ghq-cloned repository. That makes them harder to find via fzf, so be careful.\nWith the configuration above, running gwq add feature-branch produces something like:\n~/ghq/ github.com/shunk031/app # original repo (ghq) github.com/shunk031/app=feature-auth # worktree (gwq) github.com/shunk031/app=bugfix-login # worktree (gwq) ... github.com/shunk031/infra # another repo (ghq) github.com/shunk031/infra=refactor-tf # worktree (gwq) Creating a â€œJumpâ€ â€¦","date":1768574726,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a658c219c8b0837c33702e8b97df8d2f","permalink":"https://shunk031.me/post/ghq-gwq-fzf-worktree/","publishdate":"2026-01-16T23:45:26+09:00","relpermalink":"/post/ghq-gwq-fzf-worktree/","section":"post","summary":"Coding agents thrive on parallelism, and by unifying repository clones and git worktrees under a single `ghq` root and navigating them with `fzf`, you can create a workflow thatâ€™s faster, cleaner, and friendlier for both humans and AI.","tags":["git","ghq","gwq","fzf","claudecode"],"title":"A Coding-Agent-Friendly Environment Is Friendly to Humans Too: ghq Ã— gwq Ã— fzf","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Tech Blog","Conference Report"],"content":"Our Paper and Workshop Accepted at ICCV 2025, the Top International Conference on Computer Vision (Participation Report) Hello, Iâ€™m Shunsuke Kitada (@shunk031), and I work at LY Corporation on research and development for image generation and design generation.\nFrom October 19 to 23, 2025, I attended and presented at the International Conference on Computer Vision, ICCV 2025, held in Hawaii, USA.\nIn this article, Iâ€™ll share insights gained from ICCV workshops and the main conference. First, Iâ€™ll report on the workshop we organized, where I and other LY employees led international discussions, and introduce the insights gained from a workshop specialized in advertising and design generation. Then, Iâ€™ll analyze the latest trends at the main conferenceâ€”such as acceleration and controllability of diffusion models, and hierarchical / structured design generationâ€”alongside an overview of our own research teamâ€™s presentation.\nEntrance of the venue This article is an English translation of the following Japanese article: Report on the FOUND Workshop Workshop Overview and Activities as Organizers Our workshop, titled Foundation Data for Vision: Challenges and Opportunities (FOUND), was organized by LY employees including myself (Kitada) and Dr. Komatsu, and was accepted at ICCV 2025.\nOur companyâ€™s news page about the workshop acceptance FOUND workshop homepage The theme centers on â€œdata,â€ which is indispensable for recent advances in foundation models, and the workshop aimed to provide an international forum to discuss the challenges and opportunities around it.\nThis acceptance represents one of the rare cases where a workshop at ICCV, one of the worldâ€™s top-tier conferences, is led by Japanese researchers. It demonstrates that research and industrial efforts originating in Japan are being recognized on a global stage.\nLYâ€™s Contribution and On-Site Highlights LY Corporation took the lead in organizing this workshop. Through this, we were able to demonstrate, at least to some extent, that we are in a position to drive international discussions on foundation data, an urgent topic that will shape the future of the computer vision field.\nFOUND workshop opening. Opening session by lead organizer Yoshihiro Fukuhara. FOUND workshop sponsors. LY Corporation (LINEãƒ¤ãƒ•ãƒ¼) was also listed as a sponsor. The workshop attracted many researchers and practitioners from both academia and industry, leading to lively technical exchanges. In particular, the joint social event we held together with the LIMIT Workshop, also at ICCV 2025, was extremely successful. It served as a valuable opportunity to foster deeper networking among participants and to expand the possibilities for future international collaborations.\nLIMIT \u0026amp; FOUND workshop banquet. . Researchers from Google, OpenAI, Salesforce Research, who gave invited talks at the workshops, as well as many members from universities and research institutions joined us. Based on this experience as organizers, if you are considering hosting a workshop at an international conference, or if you feel uncertain about how to write a proposal or how to manage operations, please feel free to reach out. In particular, Mr. Fukuhara, who served as the lead organizer, and Dr. Kataoka from AIST, who led the co-hosted LIMIT Workshop, both have extensive experience and can provide concrete advice. I (Kitada) would also be happy to support where I can. I sincerely hope that more researchers and engineers from Japan will take the lead in discussions at international conferences.\nInsights from Attended Workshops From the perspective of my domain of responsibility and expertiseâ€”image generation, design generation, and business impact (especially in advertising and marketing), I attended and will report on two workshops.\nReport on â€œComputer Vision in Advertising and Marketing (CVAM)â€ The CVAM Workshop focused on the latest applications of computer vision (CV) in the fields of digital advertising and marketing.\nCVAM workshop page From a business impact standpoint, the workshopâ€™s theme, applications of CV technologies in advertising and marketing, specifically covered creative generation, optimization of marketing systems, brand intelligence, and more.\nIn our business, which aims to maximize advertising effectiveness, understanding and generating visual content is an urgent challenge. By participating in CVAM, I was able to reconfirm the importance of discussions on how to connect the latest image generation technologies not only to creative production, but also to ad effectiveness measurement and prediction of consumer behavior.\nReport on â€œWorkshop on Graphic Design Understanding and Generation 2025 (GDUG)â€ The GDUG Workshop aimed to discuss key concepts, technical constraints, and ethical aspects in the recognition and generation of graphic design and documents.\nGDUG workshop page From the perspective of design generation, a shared concern was that, while many research efforts focus on pixel-based image â€¦","date":1764028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"b57666e3212bce16b70a9af1b45e3a83","permalink":"https://shunk031.me/post/iccv2025-report/","publishdate":"2025-11-25T00:00:00Z","relpermalink":"/post/iccv2025-report/","section":"post","summary":"At ICCV 2025, our team from LY Corp. successfully organised and participated in a workshop and paper presentation, gaining deep insights into the shift of computerâ€vision research toward structured, controllable design generation and fast, reliable foundation-model technologies.","tags":["ICCV2025","Conference Report","Computer Vision","Design Generation","Foundation Models"],"title":"ICCV2025 Conference Participation Report","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":"å …ç‰¢.py #1 å …ç‰¢.py ã¯Pythonã‚’å®‰å…¨ã«æ‰±ã†ãŸã‚ã®æŠ€è¡“ã«èˆˆå‘³ã‚’æŒã¤ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®ã‚¤ãƒ™ãƒ³ãƒˆã§ã™ã€‚ä»Šå›ãŒåˆé–‹å‚¬ã§ã™!!\nè³‡æ–™ - Slides ","date":1763631000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"97152797c4fb4de457a1cb1af1032a8e","permalink":"https://shunk031.me/event/kenro-py-01-invited-talk/","publishdate":"2025-11-20T00:00:00+09:00","relpermalink":"/event/kenro-py-01-invited-talk/","section":"event","summary":"pytest ã¨ã€Œæœ€åˆã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã€ã¨ã„ã†è€ƒãˆæ–¹ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã«å†ç¾æ€§ã¨è¨­è¨ˆã®æ˜ç¤ºæ€§ã‚’ç¢ºä¿ã™ã‚‹æ‰‹æ³•ã‚’ã€ä¸»ã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã®ãŒå„„åŠ«ã ãªã¨æ„Ÿã˜ã¦ã„ã‚‹ç ”ç©¶è€…ãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã«å‘ã‘ã¦ç´¹ä»‹ã—ã¾ã™","tags":["Invited Talk","pytest","TDD","Machine Learning"],"title":"[Invited Talk] ãƒ†ã‚¹ãƒˆã‚’æ›¸ã‹ãªã„ç ”ç©¶è€…ã«é€ã‚‹ â€œæœ€åˆã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ãâ€ å®Ÿé¨“ã‚³ãƒ¼ãƒ‰å…¥é–€ â€“ ã‚ªãƒ¬ã‚ªãƒ¬æœ€å¼· main.py ã‹ã‚‰æŠœã‘å‡ºã™ãŸã‚ã« â€“","type":"event"},{"authors":["æ°¸äº• å¤§åœ°","å®ˆç”° ç«œæ¢§","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Computer Vision","Image Generation","Layout-to-Image Generation","Noise Optimization"],"content":"","date":1762959600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"9301a7e58e3f8f83c5ef5d41eb5c6766","permalink":"https://shunk031.me/publication/nagai2025ibis/","publishdate":"2025-11-13T00:00:00+09:00","relpermalink":"/publication/nagai2025ibis/","section":"publication","summary":"ç¬¬ 28 å› æƒ…å ±è«–çš„å­¦ç¿’ç†è«–ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ï¼Œ2025.","tags":["Domestic Conference","Non-refereed","Image Generation","Diffusion Model","IBIS"],"title":"æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæŒ‡å®šç”Ÿæˆã«å‘ã‘ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŸºã¥ãåˆæœŸãƒã‚¤ã‚ºæœ€é©åŒ–","type":"publication"},{"authors":["Daichi Nagai","Ryugo Morita","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":[],"content":"","date":1762246200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"112b0aad566d1c19405314e2381edc08","permalink":"https://shunk031.me/publication/nagai2025taue/","publishdate":"2025-11-04T17:50:00+09:00","relpermalink":"/publication/nagai2025taue/","section":"publication","summary":"Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.","tags":["Preprint","Non-Refereed","Layer-wise Image Generation","Diffusion Model","International Publication"],"title":"TAUE: Training-free Noise Transplant and Cultivation Diffusion Model","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":[],"content":" Table of Contents Introduction The difficulty of a dynamically typed language What are type hints? Example 1: Basic type hints for function arguments and return values Example 2: When the return value may be one of several types Example 3: Type hints for collection types Example 4: Optional type Example 5: Callable, Any, and other type hints Benefits of using type hints 1) Improved readability \u0026amp; maintainability 2) Early detection of errors via static type checkers \u0026amp; IDEs 3) Improved code completion 4) Benefits apply from largeâ€scale development to small scripts \u0026amp; experimental code 5) Synergy with major libraries \u0026amp; frameworks Practical steps: Points to introduce type hints 1) Donâ€™t try to annotate everything at once 2) Start with the highestâ€priority areas 3) Use IDEs and static type checkers 4) Apply even in experimental code or prototype 5) Allow partial annotation 6) Decide on style guides \u0026amp; rules Summary and the next step This article is a supplementary piece for the column â€œOn Python type hints that save everythingâ€ in Pythonã§å­¦ã¶ç”»åƒç”Ÿæˆ. In this column, we compactly introduce the background of why Python type hints were born, their advantages, and how to use them. If youâ€™ve postponed it thinking â€œit looks a bit bothersomeâ€¦â€, once you actually use it youâ€™ll realise itâ€™s a reliable element that can significantly raise development efficiency for both teams and individuals. First, letâ€™s start with understanding the meaning of using type hints, and introduce them in a relaxed way.\nIntroduction Python is widely loved as a language thanks to its ease of writing code and low learning cost. However, as development progresses, have you ever faced issues like â€œwhat type was this variable again?â€ or â€œwhat kind of return value did this function have?â€ Especially in team development, or when reading your own code after several days or months, lack of type information can lead to major confusion.\nWhat we pay attention to here is type hints. Type hints provide developers with the â€œhint that this variable is assumed to be this typeâ€, and can significantly improve readability and maintainability. Since type hints donâ€™t force strict checking at runtime, one of their appealing features is that you can introduce them without losing the â€œdynamic typed ease-of-useâ€. In this article we compactly introduce how Python type hints came about, their benefits, and how to use them. Letâ€™s begin by understanding the significance of type hints and starting with a relaxed introduction.\nThe difficulty of a dynamically typed language Python, as a dynamically typed language, has the advantage that you donâ€™t always have to worry about types when writing code, which makes it easy for many developers to use. However, as code becomes more complex, you often encounter questions such as â€œwhat kind of value was this variable originally thinking of?â€ or â€œwhat does this function return?â€ Particularly in the following kinds of cases, you might not realise a bug until you execute the code.\nFor example, consider a simple case:\ndef greet(name): return \u0026#34;Hello, \u0026#34; + name print(greet(\u0026#34;Alice\u0026#34;)) # \u0026#34;Hello, Alice\u0026#34; print(greet(42)) # TypeError: Can\u0026#39;t convert \u0026#39;int\u0026#39; object to str implicitly Although this code may look harmless, if you pass an integer as the argument when a string is assumed, a runtime error occurs. In smallâ€scale code you can easily find the cause, but in largeâ€scale or longâ€term projects, or when dealing with code written by someone else, it takes time to locate where a type mismatch occurred.\nMoreover, the issue unique to dynamically typed languages â€” â€œtype mismatch or unintended type conversion discovered laterâ€ â€” can greatly slow down development speed. There are more and more situations where you donâ€™t know until you run the code, and in major refactoring or collaborations confusion easily arises.\nAs seen in the relationship between JavaScript and TypeScript, an approach of introducing the concept of types later into a dynamically typed language to use it more safely has become commonplace. In Python as well, type hints are drawing attention as a method to explicitly provide type information while preserving the ease of a dynamically typed language. This leads to the importance of type hints described next.\nWhat are type hints? Even in dynamically typed Python, there is a mechanism where you can annotate in the source â€œwhat type the argument or return value assumesâ€ by writing it, thereby improving development efficiency and maintainability. This is called type hints. Since Python 3.5, based on PEP 484, the specification was introduced, and by using the typing module, you can explicitly declare typeâ€related information in your source code.\nType hints function purely as â€œhintsâ€, and Python does not perform strict type checking at runtime. However, static type checkers (for example: mypy/pyright) or IDE built-in autocomplete features use type hints to warn incorrect usage and deliver more accurate code completion. Below are several usage examples â€¦","date":1761075757,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"fb9a024b01c1fa9f6738c1441141600d","permalink":"https://shunk031.me/post/saving-everything-python-typehint/","publishdate":"2025-10-21T19:42:37Z","relpermalink":"/post/saving-everything-python-typehint/","section":"post","summary":" Table of Contents Introduction The difficulty of a dynamically typed language What are type hints? Example 1: Basic type hints for function arguments and return values Example 2: When the return value may be one of several types Example 3: Type hints for collection types Example 4: Optional type Example 5: Callable, Any, and other type hints Benefits of using type hints 1) Improved readability \u0026 maintainability 2) Early detection of errors via static type checkers \u0026 IDEs 3) Improved code completion 4) Benefits apply from largeâ€scale development to small scripts \u0026 experimental code 5) Synergy with major libraries \u0026 frameworks Practical steps: Points to introduce type hints 1) Donâ€™t try to annotate everything at once 2) Start with the highestâ€priority areas 3) Use IDEs and static type checkers 4) Apply even in experimental code or prototype 5) Allow partial annotation 6) Decide on style guides \u0026 rules Summary and the next step This article is a supplementary piece for the column â€œOn Python type hints that save everythingâ€ in Pythonã§å­¦ã¶ç”»åƒç”Ÿæˆ. In this column, we compactly introduce the background of why Python type hints were born, their advantages, and how to use them. If youâ€™ve postponed it thinking â€œit looks a bit bothersomeâ€¦â€, once you actually use it youâ€™ll realise itâ€™s a reliable element that can significantly raise development efficiency for both teams and individuals. First, letâ€™s start with understanding the meaning of using type hints, and introduce them in a relaxed way.\n","tags":[],"title":"On Python Type Hints that Save Everything!","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["tech"],"content":"This article explains an approach to dotfiles management that emphasizes testability, using the authorâ€™s dotfiles repository shunk031/dotfiles as a case study.\nTable of Contents Introduction Dotfiles and Dotfiles Repositories The Problem of Untested Scripts My Repositoryâ€™s Approach: Testable Configuration Architecture Design: Testable Configuration Repository Structure Design Philosophy Test \u0026amp; CI/CD Strategy Unit Test Implementation with Bats Comprehensive Verification with GitHub Actions Implementation Details and Operational Flow Structure and Implementation Examples of Setup Scripts Development and Maintenance Flow Conclusion Introduction Dotfiles and Dotfiles Repositories The dotfiles refer to configuration files that start with a â€œ.â€ (dot) such as .bashrc, .vimrc, and .gitconfig. In recent years, dotfiles repositories that manage these files using Git repositories have become widely popular among developers.\nThe dotfiles repositories often function not just as configuration file management tools, but as automated development environment setup tools that include configuration files, installation scripts, and setup scripts. This enables quick and consistent setup on new machines and environments.\nThe Problem of Untested Scripts Most setup and installation scripts included in dotfiles repositories are not tested for proper functionality (which is painful). As a result, various problems can occur when setting up in new environments. Script errors, installation failures of some tools due to dependency issues, and script malfunctions due to OS updates may go unnoticed until actually executed. Itâ€™s extremely stressful when you get a new computer or environment and excitedly start the setup process, only to have errors occur midway through, preventing the environment setup from completing.\nThis lack of quality assurance results in what should be automated environment construction consuming a lot of time on manual problem-solving and debugging.\nMy Repositoryâ€™s Approach: Testable Configuration To solve the above problems, my dotfiles repository builds an architecture that emphasizes testability. Setup scripts are managed as independent files to enable individual testing, quality is ensured through automated testing with Bats, and continuous testing and code coverage measurement are performed in macOS and Ubuntu environments using GitHub Actions.\nFor managing dotfiles, Iâ€™ve adopted chezmoi. chezmoi is a modern dotfiles management tool with high popularity on GitHub (10,000+â­ï¸). Written in Go as a single, dependency-free binary, chezmoi is easy to install even on a brand-new, clean environment.\nEnvironment setup on a new machine can be executed with the following very simple one-liner using chezmoiâ€™s official installer1.\nsh -c \u0026#34;$(curl -fsLS get.chezmoi.io)\u0026#34; -- init --apply $GITHUB_USERNAME Environment-specific settings can be dynamically generated using chezmoiâ€™s template functionality based on Goâ€™s text/template as follows.\n[user] # Can be dynamically specified via template functionality name = \u0026#34;{{.name}}\u0026#34; # - User name email = \u0026#34;{{.email}}\u0026#34; # - Email address etc. {{- if eq .chezmoi.os \u0026#34;darwin\u0026#34;}} # macOS-specific settings [credential] helper = osxkeychain {{- end}} In this way, we aim to achieve reliable dotfiles management by ensuring script quality through testable configuration and flexibly managing environment-specific settings through chezmoiâ€™s template functionality.\nArchitecture Design: Testable Configuration Repository Structure My repository is broadly divided into three directories: home/, install/, and tests/, managing dotfiles, environment setup scripts, and automated tests independently.\n. â”œâ”€â”€ ... â”‚ â”œâ”€â”€ home/ # dotfiles under chezmoi management â”‚ â”œâ”€â”€ dot_bashrc # - deployed as ~/.bashrc â”‚ â”œâ”€â”€ dot_vimrc # - deployed as ~/.vimrc â”‚ â”œâ”€â”€ dot_config/ # - deployed as ~/.config/ â”‚ â””â”€â”€ .chezmoi.yaml.tmpl # - chezmoi configuration file â”‚ â”œâ”€â”€ install/ # setup scripts (testable) â”‚ â”œâ”€â”€ common/ # - common installation scripts â”‚ â”œâ”€â”€ macos/ # - macOS-specific scripts â”‚ â””â”€â”€ ubuntu/ # - Ubuntu-specific scripts â”‚ â”œâ”€â”€ tests/ # automated tests with Bats â”‚ â”œâ”€â”€ install/ # - tests for installation scripts â”‚ â””â”€â”€ files/ # - tests for files after chezmoi deployment â”‚ â””â”€â”€ ... Design Philosophy The core of this architecture lies in â€œseparation of concernsâ€ and â€œmaximizing testabilityâ€. Traditional dotfiles repositories mix configuration files and setup scripts, making testing difficult, but this configuration clearly separates each element.\nThe install/ directory: Easy Unit Testing Through Script Separation By making setup scripts independent from chezmoi, individual testing becomes possible.\ninstall/common/rust.sh: Installation of tools used commonly across machines (e.g., Rust) install/macos/common/brew.sh: Installation of Homebrew used commonly on macOS install/ubuntu/common/misc.sh: Installation of tools used commonly on Ubuntu (e.g., curl, jq) Platform-specific configuration separates OS-specific logic, allowing each to be â€¦","date":1759748400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"5d8456854878f83db252650b947a94cb","permalink":"https://shunk031.me/post/testable-dotfiles-management-with-chezmoi/","publishdate":"2025-10-06T20:00:00+09:00","relpermalink":"/post/testable-dotfiles-management-with-chezmoi/","section":"post","summary":"This article explains an approach to dotfiles management that emphasizes testability, using the author's dotfiles repository shunk031/dotfiles as a case study.","tags":["dotfiles","chezmoi","Bats","GitHub Actions"],"title":"Testable Dotfiles Management With Chezmoi","type":"post"},{"authors":["Yoshihiro Fukuhara","Hirokatsu Kataoka","PÃ¼ren GÃ¼ler","Shunsuke Kitada, Ph.D.","Xavier Boix","Dan Hendrycks","Keisuke Tateno","Shinichi Mae","Tatsuya Komatsu","Nishant Rai","Ryo Nakamura","Risa Shinoda","Takahiro Itazuri","Yoshiki Kubotani","Guarin FlÃ¼ck","Wadim Kehl","Kazuki Kozuka"],"categories":["Workshop","ICCV2025"],"content":"We are happy to announce the ICCV 2025 workshop â€œFOUND: Foundation Data for Industrial Tech Transferâ€ has been accepted! This workshop will be held in conjunction with ICCV 2025, one of the top conferences in computer vision. The workshop will highlight the critical role of data in building foundation models, with a particular focus on robust dataset construction and industry-driven applications.\nAbout FOUND Workshop Recently, Transformer-based foundation models have achieved outstanding performance across a broad spectrum of benchmarks spanning recognition and generation tasks, and their versatility has fueled rapid advances in both AI research and industrial deployment. To seamlessly adapt these models to downstream tasks in diverse real-world domains-including medicine, manufacturing, robotics, and the creative industries-and thereby deliver tangible impact on human life, it is indispensable to develop Tech Transfer technologies that encompass domain-specific fine-tuning and robust MLOps pipelines, where the decisive factor is the breadth and quality of data available for those tasks. At the same time, model evaluation is approaching saturation on conventional IID benchmarks, prompting growing calls to redesign evaluation metrics and tasks that dispense with the IID assumption and explicitly capture out-of-distribution (OOD) and long-tail phenomena. Advancing both (i) Tech Transfer to heterogeneous downstream tasks and (ii) the definition of next-generation evaluation criteria therefore hinges on curating and exploiting broader and deeper data resources-Foundation Data, as we term them. Against this backdrop, the ICCV 2025 workshop â€œFOUND: Foundation Data for Industrial Tech Transferâ€ will convene researchers from industry and academia to share techniques for realizing Foundation Data and to engage in comprehensive discussions on model adaptation and the design of novel evaluation tasks grounded in such data, with the ultimate aim of opening new horizons for AI research and application.\nTopics of Interest The workshop focus on following topics across the diverse domains covered by our organizers:\nData-centric approach\nData collection from real-world/non-Internet domain Data curation from rough data in the wild into sophisticated and labeled datasets Data crawling, annotation, and cleansing for training of AI models Data labeling with {Self, Semi, Weakly, Un}-supervised learning without any human instructions Data generation with synthetic, graphics, and artificial images Data and a new evaluation setup for assessing powerful foundation models Data representation and benchmarks for new sensing modalities (e.g., event cameras, multimodal sensors) Tech transfer approach\nAdaptation techniques from large-/middle-scale AI models Evaluation techniques to assess the optimal AI models for complex downstream tasks {Few-shot, Zero-shot} learning techniques {Out-of, Long-tail} distribution scenarios Security, privacy, ethics, and accountability toward real-world applications Model adaptation strategies for novel sensor data and multimodal fusion ","date":1758942827,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0db995804d86b34c48b16646902e999e","permalink":"https://shunk031.me/post/iccv2025-found-workshop/","publishdate":"2025-09-27T03:13:47Z","relpermalink":"/post/iccv2025-found-workshop/","section":"post","summary":"https://iccv2025-found-workshop.limitlab.xyz/","tags":["Workshop","ICCV2025"],"title":"Foundation Data for Industrial Tech Transfer","type":"post"},{"authors":["ç«¹ä¸‹ ç†æ–—","å·ç”° æ‹“æœ—","å¤§æ©‹ å·§","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI Agent"],"content":"","date":1758282052,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"57c4a20049c3240601a8f4cf59e1137f","permalink":"https://shunk031.me/publication/takeshita2025yans/","publishdate":"2025-09-16T00:00:00+09:00","relpermalink":"/publication/takeshita2025yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 20 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["AI Agent","Computer Use","Domestic Conference","Non-refereed","YANS"],"title":"ãƒ­ãƒ¼ã‚«ãƒ« LLM ã‚’ç”¨ã„ãŸ AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¾çŠ¶ã¨èª²é¡Œ","type":"publication"},{"authors":["é™³ å† å¸†","ç”°ä¸­ å„ªå¤ªæœ—","ä¸­å· ç¿¼","åŒ—ç”° ä¿Šè¼”","å·ç”° æ‹“æœ—","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Sentiment Analysis"],"content":"","date":1758260400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"7fe1e6c21dc465e6809ab0bf2d3cb352","permalink":"https://shunk031.me/publication/chan2025yans/","publishdate":"2025-09-16T00:00:00+09:00","relpermalink":"/publication/chan2025yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 20 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["Natural Language Processing","Sentiment Analysis","Domestic Conference","Non-refereed","YANS"],"title":"WRIME-TCï¼šæ™‚é–“çš„æ–‡è„ˆã«ã‚ˆã‚‹æ›¸ãæ‰‹ã¨èª­ã¿æ‰‹ã®æ„Ÿæƒ…åˆ†æã®å¼·åŒ–","type":"publication"},{"authors":["ä¸­ç”º ç¤¼æ–‡","æµ¦å®— é¾ç”Ÿ","å‰æ©‹ äº®å¤ª","å’Œç”° æœ‰è¼ä¹Ÿ","åŒ—ç”° ä¿Šè¼”","ç‰§ç”° å…‰æ™´"],"categories":["Vision \u0026 Language","Computational Advertising"],"content":"","date":1758249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2c6bd396871551cb625569342aca4111","permalink":"https://shunk031.me/publication/nakamachi2025yans/","publishdate":"2025-09-16T00:00:00+09:00","relpermalink":"/publication/nakamachi2025yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 20 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["Computational Advertising","Typography Analysis","Domestic Conference","Non-refereed","YANS"],"title":"æ—¥æœ¬èªã®åºƒå‘Šç”»åƒå‘ã‘ã‚¿ã‚¤ãƒã‚°ãƒ©ãƒ•ã‚£å±æ€§ã®ç”Ÿæˆå‹è§£æï¼šå°è¦æ¨¡ VLM ã® LoRA å¾®èª¿æ•´ã«ã‚ˆã‚‹æ¤œè¨¼","type":"publication"},{"authors":["å·ç”° æ‹“æœ—","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1758171600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"3764079878f801272fb3b7db9a7286b3","permalink":"https://shunk031.me/publication/kawada2025yans/","publishdate":"2025-09-16T00:00:00+09:00","relpermalink":"/publication/kawada2025yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 20 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["AI for Science","Contrastive Learning","Domestic Conference","Non-refereed","YANS"],"title":"GenGAï¼šå­¦è¡“è«–æ–‡ã«ãŠã‘ã‚‹ç·¨é›†å¯èƒ½ãª Graphical Abstract ã®è‡ªå‹•ç”Ÿæˆã«é–¢ã™ã‚‹åˆæœŸæ¤œè¨","type":"publication"},{"authors":["Jiahao Zhang","Ryota Yoshihashi","Shunsuke Kitada, Ph.D.","Atsuki Osanai","Yuta Nakashima"],"categories":["Layout Generation","Creative Graphic Design"],"content":"","date":1753974000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"87aeababb381a3fdf0171427edfcb339","permalink":"https://shunk031.me/publication/zhang2025miru/","publishdate":"2025-07-28T00:00:00+09:00","relpermalink":"/publication/zhang2025miru/","section":"publication","summary":"ç¬¬ 28 å› ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["Domestic Conference","Non-refereed","MIRU"],"title":"VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction","type":"publication"},{"authors":["å·ç”° æ‹“æœ—","åŒ—ç”° ä¿Šè¼”","æ ¹æœ¬ é¢¯æ±°","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1753974000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2d4608049c8ce92835c1dc3471a82b78","permalink":"https://shunk031.me/publication/kawada2025miru/","publishdate":"2025-07-28T00:00:00+09:00","relpermalink":"/publication/kawada2025miru/","section":"publication","summary":"ç¬¬ 28 å› ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2025.","tags":["Domestic Conference","Non-refereed","MIRU"],"title":"ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆæ¨è–¦ã¨è©•ä¾¡ã®çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"Iâ€™m thrilled to announce that, in addition to my main role at LYCorp., Iâ€™ve just started as an part-time researcher at The University of Electro-Communications! Under the guidance of Prof. Satoshi Hara, Iâ€™ll be working on the K Program â€œRed Teaming Framework for Misalignment in Large Language Models.â€ Looking forward to making an impact in both industry and academia!\n","date":1747580400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"06d02ff5dcb5b8a05131cdacba93859c","permalink":"https://shunk031.me/news/appointed-uectokyo-kprogram-llm-research/","publishdate":"2025-05-19T00:00:00+09:00","relpermalink":"/news/appointed-uectokyo-kprogram-llm-research/","section":"news","summary":"Iâ€™m thrilled to announce that, in addition to my main role at LYCorp., Iâ€™ve just started as an part-time researcher at The University of Electro-Communications! Under the guidance of Prof. Satoshi Hara, Iâ€™ll be working on the K Program â€œRed Teaming Framework for Misalignment in Large Language Models.â€ Looking forward to making an impact in both industry and academia!\n","tags":["News"],"title":"Appointed as Researcher at The University of Electro-Communications","type":"news"},{"authors":["Yoshihiro Fukuhara","Hirokatsu Kataoka","PÃ¼ren GÃ¼ler","Shunsuke Kitada, Ph.D.","Xavier Boix","Dan Hendrycks","Keisuke Tateno","Shinichi Mae","Tatsuya Komatsu","Nishant Rai","Ryo Nakamura","Risa Shinoda","Takahiro Itazuri","Yoshiki Kubotani","Guarin FlÃ¼ck","Wadim Kehl","Kazuki Kozuka"],"categories":["News"],"content":"Our workshop proposal â€œFOUND: Foundation Data for Foundation Modelsâ€ has been accepted at ICCV 2025, one of the top conferences in computer vision. This workshop will highlight the critical role of data in building foundation models, with a particular focus on robust dataset construction and industry-driven applications.\nMore Information about the Workshop\nIt is a great honor to serve as an organizer of this workshop. Notably, it is quite rare for a workshop led by researchers from Japan to be accepted at such a premier international venue, making this achievement especially meaningful.\n","date":1744934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2f7f9dde47df9aba0bfd768a4102779f","permalink":"https://shunk031.me/news/acceptance-to-iccv2025-found-workshop/","publishdate":"2025-04-18T00:00:00Z","relpermalink":"/news/acceptance-to-iccv2025-found-workshop/","section":"news","summary":"Our workshop proposal â€œFOUND: Foundation Data for Foundation Modelsâ€ has been accepted at ICCV 2025, one of the top conferences in computer vision. This workshop will highlight the critical role of data in building foundation models, with a particular focus on robust dataset construction and industry-driven applications.\n","tags":["News"],"title":"Accepted our Workshop Proposal on Foundation Data to ICCV 2025","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"Iâ€™m thrilled to announce the release of my new book, â€œImage Generation with Pythonâ€! With the rapid advancements in technology, image generation has become more accessible than ever. This comprehensive guide delves into the fascinating world of image generation, focusing on diffusion models and Stable Diffusion ğŸŒŸ\nWhether youâ€™re an AI engineer, a student interested in machine learning, or a creative professional looking to harness AI in your field, this book is crafted just for you. It offers a deep dive into both the theory and practical skills needed to excel in this cutting-edge domain.\nPlease note: This book is available in Japanese only.\nDiscover more about the book by checking out the link below!\n","date":1742864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"6cb13edcd11edd46c2754ae75e8119d0","permalink":"https://shunk031.me/news/published-image-generation-with-python/","publishdate":"2025-03-25T10:00:00+09:00","relpermalink":"/news/published-image-generation-with-python/","section":"news","summary":"Iâ€™m thrilled to announce the release of my new book, â€œImage Generation with Pythonâ€! With the rapid advancements in technology, image generation has become more accessible than ever. This comprehensive guide delves into the fascinating world of image generation, focusing on diffusion models and Stable Diffusion ğŸŒŸ\n","tags":["News"],"title":"Published \"Image Generation with Python\" from Impress Books","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Book"],"content":" ","date":1742864400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"98d4febe00834516c71c2f8572dfc026","permalink":"https://shunk031.me/post/image-generation-with-python/","publishdate":"2025-03-25T10:00:00+09:00","relpermalink":"/post/image-generation-with-python/","section":"post","summary":"https://book.impress.co.jp/books/1123101104","tags":["Book"],"title":"æ›¸ç±ã€ŒPythonã§å­¦ã¶ç”»åƒç”Ÿæˆã€","type":"post"},{"authors":["å·ç”° æ‹“æœ—","æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1741649400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ee7e62b34100135c41989b6e321bf213","permalink":"https://shunk031.me/publication/kawada2025nlp/","publishdate":"2025-03-03T00:00:00+09:00","relpermalink":"/publication/kawada2025nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 31 å›å¹´æ¬¡å¤§ä¼šï¼Œ2025.","tags":["Domestic Conference","Non-refereed","AI for Science","Natural Language Processing","Computer Vision","Vision \u0026 Language","ANLP","NLP2025"],"title":"SciGA: å­¦è¡“è«–æ–‡ã«ãŠã‘ã‚‹ Graphical Abstract è¨­è¨ˆæ”¯æ´ã®ãŸã‚ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ","type":"publication"},{"authors":["æ°¸äº• å¤§åœ°","å®ˆç”° ç«œæ¢§","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Computer Vision","Image Generation"],"content":"","date":1741446000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"14d829142a440ee347af8d18b039b198","permalink":"https://shunk031.me/publication/nagai2025ipsj/","publishdate":"2025-03-09T00:00:00+09:00","relpermalink":"/publication/nagai2025ipsj/","section":"publication","summary":"æƒ…å ±å‡¦ç†å­¦ç¬¬ 87 å›å…¨å›½å¤§ä¼šï¼Œ2025.","tags":["Domestic Conference","Non-refereed","Image Generation","Diffusion Model","IPSJ","IPSJ2025"],"title":"æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ç”Ÿæˆå¯¾è±¡ã®å€‹åˆ¥é…ç½®ã¨ç”Ÿæˆã«ã‚ˆã‚‹ç”»è³ªæ”¹å–„ã®è©¦ã¿","type":"publication"},{"authors":["Takuro Kawada","Shunsuke Kitada, Ph.D.","Sota Nemoto","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1735948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d6fab03f178eeb4aa71c892a1c5827e9","permalink":"https://shunk031.me/publication/kawada2025sciga/","publishdate":"2025-07-03T00:00:00+09:00","relpermalink":"/publication/kawada2025sciga/","section":"publication","summary":"A comprehensive dataset and framework for automated graphical abstract creation in academic papers.","tags":["Preprint","Non-Refereed","AI for Science","Natural Language Processing","Computer Vision","Vision \u0026 Language","Dataset"],"title":"SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers","type":"publication"},{"authors":["Sota Nemoto","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the IEEE Access. This paper is an extended version of the content presented at the â€œPractical ML for Low Resource Settings Workshop @ ICLR 2024â€.\nSota Nemoto, Shunsuke Kitada, Hitoshi Iyatomi. â€œMajority or Minority: Data Imbalance Learning Method for Named Entity Recognitionâ€.\n","date":1735052400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0a95bc8bcbd6a6e5dcd30c6125b2fa71","permalink":"https://shunk031.me/news/acceptance-to-ieee-access-nemoto2024majority/","publishdate":"2024-12-25T00:00:00+09:00","relpermalink":"/news/acceptance-to-ieee-access-nemoto2024majority/","section":"news","summary":"The following paper has been accepted to the IEEE Access. This paper is an extended version of the content presented at the â€œPractical ML for Low Resource Settings Workshop @ ICLR 2024â€.\n","tags":["News"],"title":"Accepted our journal paper to IEEE Access","type":"news"},{"authors":["Sota Nemoto","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Named Entity Recognition","Imbalanced Dataset"],"content":"This paper is an extended version of the content presented at the â€œPractical ML for Low Resource Settings Workshop @ ICLR 2024â€. We are honored to receive a rating of 9, which is in the top 15% of accepted papers, and a strong accept. We also gave an oral presentation in the ICLR workshop.\narXiv IEEE SCImago ","date":1735052400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"15c1b213bcf98e10f70a0ae026d39809","permalink":"https://shunk031.me/publication/nemoto2024majority/","publishdate":"2024-12-25T00:00:00+09:00","relpermalink":"/publication/nemoto2024majority/","section":"publication","summary":"IEEE Access (**Impact Factor: 3.4** in 2024; [1st place in Engineering \u0026 Computer Science (general) at Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues\u0026hl=en\u0026vq=eng_enggeneral))","tags":["International Conference","Refereed","Oral Presentation"],"title":"Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition","type":"publication"},{"authors":["Jiahao Zhang","Ryota Yoshihashi","Shunsuke Kitada, Ph.D.","Atsuki Osanai","Yuta Nakashima"],"categories":["Layout Generation","Creative Graphic Design"],"content":"","date":1733324400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"62e1ba412ff1c6c7e2c7d5909c620cd3","permalink":"https://shunk031.me/publication/zhang2024vascar/","publishdate":"2024-12-05T00:00:00+09:00","relpermalink":"/publication/zhang2024vascar/","section":"publication","summary":"Large language models (LLMs) have proven effective for layout generation due to their ability to produce structure-description languages, such as HTML or JSON, even without access to visual information. Recently, LLM providers have evolved these models into large vision-language models (LVLM), which shows prominent multi-modal understanding capabilities. Then, how can we leverage this multi-modal power for layout generation? To answer this, we propose Visual-Aware Self-Correction LAyout GeneRation (VASCAR) for LVLM-based content-aware layout generation. In our method, LVLMs iteratively refine their outputs with reference to rendered layout images, which are visualized as colored bounding boxes on poster backgrounds. In experiments, we demonstrate that our method combined with the Gemini. Without any additional training, VASCAR achieves state-of-the-art (SOTA) layout generation quality outperforming both existing layout-specific generative models and other LLM-based methods.","tags":["Preprint","Non-Refereed","Layout Generation","Large Vision-Language Models","LYCorp","International Publication"],"title":"VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”"],"categories":null,"content":"I contributed a summary on creative graphic design to the ECCV2024 report on cvpaper.challenge group. You can find my summary on pages 75 to 81 below:\nhttps://hirokatsukataoka.net/temp/presen/241004ECCV2024Report_finalized.pdf#page=75 https://hirokatsukataoka.net/temp/presen/241004ECCV2024Report_finalized.pdf#page=81 ","date":1728269690,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0ceaf6f3339bc94d0bb7643f957dfaa8","permalink":"https://shunk031.me/event/eccv2024-report/","publishdate":"2024-10-07T11:54:50+09:00","relpermalink":"/event/eccv2024-report/","section":"event","summary":"ECCVï¼ˆEuropean Conference on Computer Visionï¼‰ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³åˆ†é‡ãƒˆãƒƒãƒ—å›½éš›ä¼šè­°ã®ã²ã¨ã¤ã§ã™ã€‚ä»Šå›ã¯åˆ†é‡ã®å‹•å‘ãƒ»æ°—ä»˜ãã‚’ã¾ã¨ã‚ã‚‹ã“ã¨ã«æ³¨åŠ›ã—ã€132ãƒšãƒ¼ã‚¸ã®è³‡æ–™ã¨ã—ã¦ä»•ä¸Šã’ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚","tags":[],"title":"ECCV 2024 é€Ÿå ±","type":"event"},{"authors":["æ°´å£ å¾³äºº","åŒ—ç”° ä¿Šè¼”","å®ˆç”° ç«œæ¢§","å½Œå†¨ ä»"],"categories":["Computer Vision","Vision \u0026 Language","Image Generation"],"content":"","date":1725526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a29cb5d10ab119bff1117d44b2e10f90","permalink":"https://shunk031.me/publication/mizuguchi2024yans/","publishdate":"2024-09-01T00:00:00+09:00","relpermalink":"/publication/mizuguchi2024yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 19 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2024.","tags":["Image Generation","Domestic Conference","Non-refereed","YANS","YANS2024"],"title":"text-to-image æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹èª˜å° attention map ã‚’ç”¨ã„ãŸç”»åƒç”Ÿæˆæ‰‹æ³•ã®ææ¡ˆ","type":"publication"},{"authors":["å°å· å‰›æ¯…","æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Audio Processing"],"content":"","date":1725526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"cd0f02c5922fb3566cf4f1c3587b7596","permalink":"https://shunk031.me/publication/ogawa2024yans/","publishdate":"2024-09-01T00:00:00+09:00","relpermalink":"/publication/ogawa2024yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 19 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2024.","tags":["Text-to-Audio Retrieval","LLM","ChatGPT","Domestic Conference","Non-refereed","YANS","YANS2024"],"title":"å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸã‚ªãƒãƒãƒˆãƒšä»˜ä¸ã«ã‚ˆã‚‹æ—¥æœ¬èªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‹¡å¼µ","type":"publication"},{"authors":["å·ç”° æ‹“æœ—","æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Vision \u0026 Language","AI for Science"],"content":"","date":1725526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"34ef12ffd646a76a476a8c02baa472b5","permalink":"https://shunk031.me/publication/kawada2024yans/","publishdate":"2024-09-01T00:00:00+09:00","relpermalink":"/publication/kawada2024yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 19 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2024.","tags":["AI for Science","Contrastive Learning","Domestic Conference","Non-refereed","YANS","YANS2024"],"title":"å­¦è¡“è«–æ–‡ã«ãŠã‘ã‚‹ Graphical Abstract è‡ªå‹•ç”Ÿæˆã®åˆæœŸæ¤œè¨","type":"publication"},{"authors":["æ°¸äº• å¤§åœ°","æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Computer Vision","Image Generation"],"content":"","date":1725526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a32edad8cd00c0953de8005078f5c89f","permalink":"https://shunk031.me/publication/nagai2024yans/","publishdate":"2024-09-01T00:00:00+09:00","relpermalink":"/publication/nagai2024yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 19 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2024.","tags":["Image Generation","Diffusion Model","Domestic Conference","Non-refereed","YANS","YANS2024"],"title":"æ½œåœ¨æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨ã„ãŸé…è‰²åˆ¶å¾¡ã®è©¦ã¿","type":"publication"},{"authors":["å²©äº• ç¿”çœŸ","é•·å†… æ·³æ¨¹","åŒ—ç”° ä¿Šè¼”","å¤§ç”º çœŸä¸€éƒ"],"categories":["Computer Vision","Layout Generation"],"content":" ","date":1722956400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"290be83c1e166e018a1dc1b75fef6979","permalink":"https://shunk031.me/publication/iwai2024miru/","publishdate":"2024-08-07T00:00:00+09:00","relpermalink":"/publication/iwai2024miru/","section":"publication","summary":"ç¬¬ 27 å› ç”»åƒã®èªè­˜ãƒ»ç†è§£ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2024.","tags":["Domestic Conference","Non-refereed","MIRU","MIRU2024"],"title":"é›¢æ•£æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®â€œå›ºç€â€ã‚’ç·©å’Œã™ã‚‹ Layout-Corrector ã®ææ¡ˆ","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":"ç¬¬61å› ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³å‹‰å¼·ä¼šï¼ é–¢æ±ï¼ˆå¾Œç·¨ï¼‰ ç›®çš„ï¼š 2024å¹´ã®6æœˆ17æ—¥ï½21æ—¥ã«ã‹ã‘ã¦é–‹å‚¬ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®ãƒˆãƒƒãƒ—ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã®ä¸€ã¤ã§ã‚ã‚‹ ã€ŒConference on Computer Vision and Pattern Recognition (CVPR) 2024ã€ã®è«–æ–‡èª­ã¿ä¼š(å¾Œç·¨)ã§ã™ã€‚\nå‰ç·¨ã¯7/7(æ—¥)ã«é–‹å‚¬äºˆå®šã§ã™ã€‚\nhttps://kantocv.connpass.com/event/321174/\nè³‡æ–™ - Slides ","date":1722667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ef07d391e1b8d93e287ecddca375dc27","permalink":"https://shunk031.me/event/kantocv-61th-cvpr-2024/","publishdate":"2024-08-03T13:00:00+09:00","relpermalink":"/event/kantocv-61th-cvpr-2024/","section":"event","summary":"In this talk, I presented the paper \"Visual Layout Composer: Image-Vector Dual Diffusion Model for Design Layout Generation\"","tags":["Journal Club","Paper Reading","Layout Generation","Poster Generation"],"title":"[Journal Club] Visual Layout Composer: Image-Vector Dual Diffusion Model for Design Layout Generation (CVPR'24)","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":" ","date":1721697000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"03f767cbd6e7caa04ae2a1b78ff65ab8","permalink":"https://shunk031.me/post/ly-new-grad-job-introduction/","publishdate":"2024-07-23T10:10:00+09:00","relpermalink":"/post/ly-new-grad-job-introduction/","section":"post","summary":"https://note.com/lycorp_recruit/n/n9d83d9ef243e","tags":["Interview","LYCorp"],"title":"æ–°å’ç¤¾å“¡ã®ä»•äº‹ç´¹ä»‹ ç”»åƒç”Ÿæˆã®ç ”ç©¶é–‹ç™ºã«æºã‚ã‚‹ãƒªã‚µãƒ¼ãƒã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆç·¨","type":"post"},{"authors":["Shoma Iwai","Atsuki Osanai","Shunsuke Kitada, Ph.D.","Shinichiro Omachi"],"categories":["Layout Generation","Creative Graphic Design"],"content":"","date":1719820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d37772f98ce23f47816e33929d09d5e6","permalink":"https://shunk031.me/publication/iwai2024layout/","publishdate":"2024-07-01T08:00:00Z","relpermalink":"/publication/iwai2024layout/","section":"publication","summary":"Proc. of the European Conference on Computer Vision (ECCV2024). (**Acceptance rate = 27.9%**)","tags":["International Conference","Refereed","Layout Generation","Discrete Diffusion Model","LYCorp","International Publication","Springer","ECCV","ECCV2024"],"title":"Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model","type":"publication"},{"authors":["Shoma Iwai","Atsuki Osanai","Shunsuke Kitada, Ph.D.","Shinichiro Omachi"],"categories":["News"],"content":"The following paper has been accepted to the ECCV2024.\nShoma Iwai, Atsuki Osanai, Shunsuke Kitada, and Shinichiro Omachi. â€œLayout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Modelâ€. ","date":1719759600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2184e40f36a0f6d09b51b6eadcad3002","permalink":"https://shunk031.me/news/acceptance-to-eccv2024/","publishdate":"2024-07-01T00:00:00+09:00","relpermalink":"/news/acceptance-to-eccv2024/","section":"news","summary":"The following paper has been accepted to the ECCV2024.\nShoma Iwai, Atsuki Osanai, Shunsuke Kitada, and Shinichiro Omachi. â€œLayout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Modelâ€. ","tags":["News"],"title":"Accepted our paper to ECCV2024","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Atsuki Osanai","Kenji Doi","Ryota Yoshihashi","Nghia Truong"],"categories":null,"content":"æŠ€è¡“ãƒãƒƒãƒ— æ–‡çŒ®æƒ…å ± ","date":1718151600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c4f99c15c346b459e83a38e22fd5e810","permalink":"https://shunk031.me/event/ssii2024-tech-map/","publishdate":"2024-06-12T00:00:00+09:00","relpermalink":"/event/ssii2024-tech-map/","section":"event","summary":"This poster session introduces our technical map that provides an overview of the field of creative graphic design AI, such as layout generation and poster generation.","tags":["Invited Talk","Invited Predentation"],"title":"[Invited Presentation] SSII2024 Technical map: Creative Design Generation (SSII'24)","type":"event"},{"authors":["Shunsuke Kitada, Ph.D.","Atsuki Osanai","Kenji Doi","Ryota Yoshihashi","Nghia Truong"],"categories":["News"],"content":"Our team provided and published the following technical map at SSII2024 session.\nğŸŒ https://bit.ly/ssii2024-techmap-creative-design-map ğŸ“ƒ https://bit.ly/ssii2024-techmap-creative-design-sheet See this post for more details.\n","date":1717945200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"827397616236bb0d35870c2fa587ef86","permalink":"https://shunk031.me/news/ssii2024-techmap/","publishdate":"2024-06-10T00:00:00+09:00","relpermalink":"/news/ssii2024-techmap/","section":"news","summary":"Our team provided and published the following technical map at SSII2024 session.\nğŸŒ https://bit.ly/ssii2024-techmap-creative-design-map ğŸ“ƒ https://bit.ly/ssii2024-techmap-creative-design-sheet See this post for more details.\n","tags":["News"],"title":"Published our technical map at SSII2024","type":"news"},{"authors":["Sota Nemoto","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted at Practical ML for Low Resource Settings Workshop (PM4LRS) @ ICLR 2024.\nSota Nemoto, Shunsuke Kitada, and Hitoshi Iyatomi. â€œMajority or Minority: Data Imbalance Learning Method for Named Entity Recognitionâ€ ","date":1709650800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"70f491615f7afedeedebf8dd2ab0a5b6","permalink":"https://shunk031.me/news/acceptance-to-iclr2024-pm4lrs/","publishdate":"2024-03-06T00:00:00+09:00","relpermalink":"/news/acceptance-to-iclr2024-pm4lrs/","section":"news","summary":"The following paper has been accepted at Practical ML for Low Resource Settings Workshop (PM4LRS) @ ICLR 2024.\nSota Nemoto, Shunsuke Kitada, and Hitoshi Iyatomi. â€œMajority or Minority: Data Imbalance Learning Method for Named Entity Recognitionâ€ ","tags":["News"],"title":"Accepted our paper to PM4LRS workshop at ICLR2024","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":"KDD2023 è«–æ–‡èª­ã¿ä¼šï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³é–‹å‚¬ï¼‰ æ¦‚è¦ 2023å¹´8æœˆã«é–‹å‚¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°é–¢é€£ã®å›½éš›å­¦è¡“ä¼šè­°ã€KDD2023ã«(29th ACM SIGKDD Conference on Knowledge Discovery \u0026amp; Data Mining) ã§ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã®èª­ã¿ä¼šã§ã™ã€‚ å‚åŠ å ±å‘Šã¨5æœ¬ã®è«–æ–‡ã®ã”ç´¹ä»‹ãŒä¸»ãªå†…å®¹ã¨ãªã‚Šã¾ã™ã€‚\nå­¦ç”Ÿã€ç¤¾ä¼šäººã€ã‚¢ã‚«ãƒ‡ãƒŸã‚¢å•ã‚ãšãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«èˆˆå‘³ã®ã‚ã‚‹æ–¹ã§ã—ãŸã‚‰ã€ã©ãªãŸã§ã‚‚ã”å‚åŠ ã„ãŸã ã‘ã¾ã™ã€‚ ãœã²ãŠæ°—è»½ã«ã”ç™»éŒ²ãã ã•ã„ï¼\nè³‡æ–™ - Slides ","date":1705487100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"860ff1b4788d43902bfd271efb1dbda5","permalink":"https://shunk031.me/event/kdd2023-reading/","publishdate":"2024-01-17T11:37:51+09:00","relpermalink":"/event/kdd2023-reading/","section":"event","summary":"In this talk, I presented the paper \"BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction\"","tags":["Journal Club","Paper Reading","CTR prediction"],"title":"[Journal Club] BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction (KDD'23)","type":"event"},{"authors":["Shunsuke Kitada, Ph.D.","Others"],"categories":["News"],"content":"A special issue of Journal of JSAI on doctoral dissertations, including a summary of my dissertation, is now available: https://www.jstage.jst.go.jp/article/jjsai/39/1/39_50/_article/-char/ja/\n","date":1704034800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"b7308158e6ac348c3f4fd1045444efba","permalink":"https://shunk031.me/news/published-jsai-phd-special-issue/","publishdate":"2024-01-01T00:00:00+09:00","relpermalink":"/news/published-jsai-phd-special-issue/","section":"news","summary":"A special issue of Journal of JSAI on doctoral dissertations, including a summary of my dissertation, is now available: https://www.jstage.jst.go.jp/article/jjsai/39/1/39_50/_article/-char/ja/\n","tags":["News"],"title":"Published a special issue of doctoral dissertation in Journal of JSAI","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Others"],"categories":[],"content":" è©²å½“ãƒšãƒ¼ã‚¸ (PDF, 0.94MB)\näººå·¥çŸ¥èƒ½å­¦ä¼šèªŒã§ã¯ï¼Œéå»1å¹´é–“ã«äººå·¥çŸ¥èƒ½ã«é–¢é€£ã™ã‚‹ç ”ç©¶ã§åšå£«ã®å­¦ä½ã‚’æˆä¸ã•ã‚ŒãŸç ”ç©¶è€…ã®æ–¹ã€…ã®åšå£«è«–æ–‡ã®æ¦‚è¦ã‚’ï¼Œãã®æ–¹ã€…ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¨ã¨ã‚‚ã«ã¾ã¨ã‚ã¦æ²è¼‰ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šï¼Œä¸–ä»£ã®ç•°ãªã‚‹ç ”ç©¶è€…é–“ã®äº¤æµã‚„ï¼Œãã®åˆ†é‡ã®ç ”ç©¶å‹•å‘ã®æŠŠæ¡ã®ä¸€åŠ©ã¨ã™ã‚‹ã“ã¨ã‚’ç‹™ã„ã¨ã—ã¦ï¼Œåšå£«è«–æ–‡æ¦‚è¦ã®ç‰¹é›†ã‚’ï¼Œæ¯å¹´1æœˆå·ã«æ²è¼‰è‡´ã—ã¦ãŠã‚Šã¾ã™ï¼\n","date":1704034800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"4b16552cd199262b3330353998dad8d5","permalink":"https://shunk031.me/publication/kitada2024jsaiphd/","publishdate":"2024-01-01T00:00:00+09:00","relpermalink":"/publication/kitada2024jsaiphd/","section":"publication","summary":"äººå·¥çŸ¥èƒ½å­¦ä¼šèªŒã®åšå£«è«–æ–‡ç‰¹é›†å·ã«å¯„ç¨¿ã—ã¾ã—ãŸã€‚","tags":["Journal","Special Issue","Non-referred"],"title":"äººå·¥çŸ¥èƒ½å­¦ä¼šèªŒ åšå£«è«–æ–‡é›† 2022.10ï½2023.9æˆä¸","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Side job"],"content":" è¬›ç¾©ã«ã¤ã„ã¦ è¬›ç¾©ãƒšãƒ¼ã‚¸ ãƒªã‚µãƒ¼ãƒã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ åŒ—ç”°ä¿Šè¼” | Coloso. https://coloso.jp/products/researchscientist-kitada-jp è¬›ç¾©è³‡æ–™ãƒšãƒ¼ã‚¸ ç”»åƒç”Ÿæˆ AI å…¥é–€ï¼šPython ã«ã‚ˆã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè·µ https://shunk031.me/coloso-python-diffusion-models/README.html ","date":1687618800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"3b0550e55ca9fa759c1203f8652c64a0","permalink":"https://shunk031.me/post/coloso-introduction-to-image-generation-ai/","publishdate":"2023-06-25T00:00:00+09:00","relpermalink":"/post/coloso-introduction-to-image-generation-ai/","section":"post","summary":"https://coloso.jp/products/researchscientist-kitada-jp","tags":["Side job"],"title":"ç”»åƒç”Ÿæˆ AI å…¥é–€ï¼šPython ã«ã‚ˆã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç†è«–ã¨å®Ÿè·µ","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Natural Language Processing","Computational Advertising","Attention Mechanism","Adversarial Training","Virtual Adversarial Training","Interpretability","Explainability","Ad Conversion Prediction","Ad Discontinuation Prediction"],"content":" ","date":1679583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e9853e9dfced5b24f8c4c5dbc294f42a","permalink":"https://shunk031.me/publication/kitada2023dissertation/","publishdate":"2023-03-24T00:00:00+09:00","relpermalink":"/publication/kitada2023dissertation/","section":"publication","summary":"Ph.D. dissertation","tags":["Thesis","Dissertation"],"title":"Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"I am thrilled to announce that I have successfully defended my PhD on March 24, 2023. The journey that began in the Intelligent Information Processing Lab has been nothing short of remarkable and exhilarating. I extend my deepest gratitude to everyone who has been an integral part of this journey.\nI want to express my sincere gratitude to my supervisor, Prof. Iyatomi \u0026amp; doctoral committee members; Dr. Nishida and Prof. Sibata, for their continuous engagement and encouragement. I also thank the Graduate School of Science and Engineering, Hosei University for for giving me advice from different perspectives and nurturing me.\nIn the lab life, I devoted all my energy to research and play under the policy â€œplay hard, study hard.â€ During my doctoral program, I was particularly supported by various people, and I would like to give back to the university and academia that have helped me after entering industry. I will aim to research and develop cutting-edge technologies for the purpose.\nThe dissertation and presentation slides can be found here.\nåšå£«ï¼ˆå·¥å­¦ï¼‰ã«ãªã‚Šã¾ã—ãŸï¼å’æ¥­å¼ã§ å…ƒæ¬…å‚46ã® @Aoi_Harada00 ã•ã‚“ã«åƒ•ã®åšè«–ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’èª­ã¿ä¸Šã’ã¦ã‚‚ã‚‰ã£ãŸã®ã§ã€äººç”Ÿã®æœ€é«˜æ½®ã«ãªã‚Šã¾ã—ãŸï¼ˆè©²å½“ç®‡æ‰€ã¯ã“ã¡ã‚‰: https://t.co/tXKUlQdxuYï¼‰ã€‚ã“ã‚Œã‹ã‚‰ã¯å’æ¥­å¼ã§ã‚¢ã‚¤ãƒ‰ãƒ«ã«åšå£«å·å–å¾—ã‚’ç¥ã£ã¦ã‚‚ã‚‰ã£ãŸã‚ªã‚¿ã‚¯ã¨ã—ã¦ç¤¾ä¼šã«å‡ºãŸã„ã¨æ€ã„ã¾ã™ğŸ¥° pic.twitter.com/tmkkyNh6CJ\nâ€” ã—ã‚…ã‚“ã‘ãƒ¼ã€ŒğŸ“•Pythonã§å­¦ã¶ç”»åƒç”Ÿæˆã€ç™ºå£²ä¸­ï¼ (@shunk031) March 26, 2023 ","date":1679583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ec9edcc8af865c31136c305b8a5d4716","permalink":"https://shunk031.me/news/phd-dissertation-defense/","publishdate":"2023-03-24T00:00:00+09:00","relpermalink":"/news/phd-dissertation-defense/","section":"news","summary":"I am thrilled to announce that I have successfully defended my PhD on March 24, 2023. The journey that began in the Intelligent Information Processing Lab has been nothing short of remarkable and exhilarating. I extend my deepest gratitude to everyone who has been an integral part of this journey.\n","tags":["News"],"title":"Successfully Defended my Ph.D. Dissertation","type":"news"},{"authors":["æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Named Entity Recognition"],"content":"","date":1678840800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"f93fbda5367f8561da3947041d150e05","permalink":"https://shunk031.me/publication/nemoto2023nlp/","publishdate":"2023-03-06T00:00:00+09:00","relpermalink":"/publication/nemoto2023nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 29 å›å¹´æ¬¡å¤§ä¼šï¼Œ2023.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2023"],"title":"Majority or Minority: å›ºæœ‰è¡¨ç¾æŠ½å‡ºã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸å‡è¡¡æ€§ã«ç€ç›®ã—ãŸæå¤±é–¢æ•°ã®ææ¡ˆ","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Hosei University Research Grant for Doctoral Course Adopters in 2022 (480,000 JPY).\n","date":1673708400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d29b360a3374cf1a70711d0e0ed80046","permalink":"https://shunk031.me/news/hosei-university-research-grant-2022/","publishdate":"2023-01-15T00:00:00+09:00","relpermalink":"/news/hosei-university-research-grant-2022/","section":"news","summary":"Got Hosei University Research Grant for Doctoral Course Adopters in 2022 (480,000 JPY).\n","tags":["News","Awards and Grants"],"title":"Got Hosei University Research Grant 2022","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Yuki Iwazaki","Riku Togashi","Hitoshi Iyatomi"],"categories":["Multi-modal model","Vision \u0026 Language","Natural Language Processing","Attention Mechanisms"],"content":"\n","date":1668351600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"192b0de8f103301ab14a1ece67f8fda6","permalink":"https://shunk031.me/publication/kitada2022dm2s2/","publishdate":"2022-11-14T00:00:00+09:00","relpermalink":"/publication/kitada2022dm2s2/","section":"publication","summary":"IEEE Access (**Impact Factor: 3.476** in 2021; [1st place in Engineering \u0026 Computer Science (general) at Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues\u0026hl=en\u0026vq=eng_enggeneral))","tags":["Journal","International Publication","Multi-modal model","Vision \u0026 Language","Refereed","Open Access","CyberAgent","IEEE"],"title":"DM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attention","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":"IR Reading 2022 ç§‹ æ¦‚è¦ æƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹åˆ†é‡ã«é–¢ã™ã‚‹ä¸»è¦å›½éš›ä¼šè­°ï¼ˆSIGIR ã‚„ WSDM ãªã©ï¼‰ã®è«–æ–‡èª­ã¿åˆã‚ã›ã‚’è¡Œã† IR Reading ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã«ã¦é–‹å‚¬ã—ã¾ã™ï¼å¤šäººæ•°ã§å”åŠ›ã—ã‚ã£ã¦è«–æ–‡ã‚’ç´¹ä»‹ã—ã‚ã†ã“ã¨ã§ï¼Œè«–æ–‡ã¸ã®ç†è§£ã‚’æ·±ã‚ãŸã‚Šï¼Œåˆ†é‡å…¨ä½“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ´ã‚“ã ã‚Šã™ã‚‹å ´ã¨ãªã‚Œã°å¹¸ã„ã§ã™ï¼ã“ã‚Œã¾ã§ã®å‹‰å¼·ä¼šã®å†…å®¹ã«ã¤ãã¾ã—ã¦ã¯ï¼Œéå»ã®ãƒšãƒ¼ã‚¸ ã‚’ã”è¦§ãã ã•ã„ï¼\nè³‡æ–™ - Slides ","date":1668231900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c0c20e735b83535710b1849f47ab105f","permalink":"https://shunk031.me/event/ir-reading-2022-autumn/","publishdate":"2022-11-12T22:17:01+09:00","relpermalink":"/event/ir-reading-2022-autumn/","section":"event","summary":"In this talk, I presented the paper \"Expressions Causing Differences in Emotion Recognition in Social Networking Service Documents\" we presented at CIKM2022.","tags":["Journal Club","Paper Reading","IR","NLP"],"title":"[Journal Club] Expressions Causing Differences in Emotion Recognition in Social Networking Service Documents (CIKM'22)","type":"event"},{"authors":["Shunsuke Kitada, Ph.D.","Yuki Iwazaki","Riku Togashi","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the IEEE Access.\nShunsuke Kitada, Yuki Iwazaki, Riku Togashi, and Hitoshi Iyatomi. â€œDM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attentionâ€ ","date":1667401200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0cff6c7a999b144019bcefaf4c0788ac","permalink":"https://shunk031.me/news/acceptance-to-ieee-access-kitada2022dm2s2/","publishdate":"2022-11-03T00:00:00+09:00","relpermalink":"/news/acceptance-to-ieee-access-kitada2022dm2s2/","section":"news","summary":"The following paper has been accepted to the IEEE Access.\nShunsuke Kitada, Yuki Iwazaki, Riku Togashi, and Hitoshi Iyatomi. â€œDM$^2$S$^2$: Deep Multi-Modal Sequence Sets with Hierarchical Modality Attentionâ€ ","tags":["News"],"title":"Accepted our journal paper to IEEE Access","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Attention Mechanisms","Virtual Adversarial Training","Text Classification","Question Answering","Natural Language Inference"],"content":" arXiv Springer SCImago ","date":1666873980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"8b53b84f70511d3572ccfec9dacaa571","permalink":"https://shunk031.me/publication/kitada2022making/","publishdate":"2022-10-27T21:33:00+09:00","relpermalink":"/publication/kitada2022making/","section":"publication","summary":"Springer Applied Intelligence (**Impact Factor: 5.019** in 2021)","tags":["Journal","International Publication","Natural Language Processing","Refereed","Springer"],"title":"Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the Springer Applied Intelligence.\nShunsuke Kitada and Hitoshi Iyatomi. â€œMaking Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Trainingâ€ ","date":1666796400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"96c9f7713992ac8670a474525f0887b8","permalink":"https://shunk031.me/news/acceptance-to-apin-kitada2022making/","publishdate":"2022-10-27T00:00:00+09:00","relpermalink":"/news/acceptance-to-apin-kitada2022making/","section":"news","summary":"The following paper has been accepted to the Springer Applied Intelligence.\nShunsuke Kitada and Hitoshi Iyatomi. â€œMaking Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Trainingâ€ ","tags":["News"],"title":"Accepted our journal paper to Springer Applied Intelligence journal","type":"news"},{"authors":["Ohata Kazuya","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Image Captioning"],"content":"","date":1666116549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ac2e7342f2febaaaf0e9cf202e13e70f","permalink":"https://shunk031.me/publication/ohata2022feedback/","publishdate":"2022-10-18T14:09:09-04:00","relpermalink":"/publication/ohata2022feedback/","section":"publication","summary":"Proc. IEEE 19th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET).","tags":["International Conference","Refereed","Natural Language Processing","Image Captioning","International Publication","HONET","HONET2022"],"title":"Feedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impaired","type":"publication"},{"authors":["Ohata Kazuya","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the IEEE HONET2022.\nOhata Kazuya, Shunsuke Kitada and Hitoshi Iyatomi. â€œFeedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impairedâ€ ","date":1665932400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"70844bc038b1615d8c430b75cd354ea2","permalink":"https://shunk031.me/news/acceptance-to-honet2022/","publishdate":"2022-10-17T00:00:00+09:00","relpermalink":"/news/acceptance-to-honet2022/","section":"news","summary":"The following paper has been accepted to the IEEE HONET2022.\nOhata Kazuya, Shunsuke Kitada and Hitoshi Iyatomi. â€œFeedback is Needed for Retakes: An Explainable Poor Image Notification Framework for the Visually Impairedâ€ ","tags":["News"],"title":"Accepted our paper to IEEE HONET2022","type":"news"},{"authors":["æ ¹æœ¬ é¢¯æ±°","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Imbalanced Dataset","Named Entity Recognition"],"content":"","date":1661785200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a3ecea35092df4b68562756da29d4604","permalink":"https://shunk031.me/publication/nemoto2022yans/","publishdate":"2022-08-30T10:10:53+09:00","relpermalink":"/publication/nemoto2022yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 17 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2022.","tags":["Named Entity Recognition","Domestic Conference","Non-refereed","YANS","YANS2022"],"title":"å›ºæœ‰è¡¨ç¾èªè­˜ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åã‚Šã«ç€ç›®ã—ãŸå‹•çš„é‡ã¿ä»˜ã‘æå¤±é–¢æ•°ã®ææ¡ˆ","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","äº•ä¸Š ç›´äºº","å¤§è°· ã¾ã‚†","å½Œå†¨ ä»"],"categories":["Vision \u0026 Language","Multi-modal Model","Document AI"],"content":"","date":1661698800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2cda6873566f73ae0f711bad32120150","permalink":"https://shunk031.me/publication/kitada2022yans/","publishdate":"2022-08-29T00:00:00+09:00","relpermalink":"/publication/kitada2022yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 17 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2022.","tags":["Vision \u0026 Language","Domestic Conference","Non-refereed","CyberAgent","YANS","YANS2022"],"title":"Document AI ã‚¿ã‚¹ã‚¯ã«å‘ã‘ãŸå¤§è¦æ¨¡äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸ Layout-aware Prompting","type":"publication"},{"authors":["Tsubasa Nakagawa","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Sentiment Analysis"],"content":" ","date":1659417492,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"10b3d624212ebfeb38afadb809c833c1","permalink":"https://shunk031.me/publication/nakagawa2022expressions/","publishdate":"2022-08-02T14:18:12+09:00","relpermalink":"/publication/nakagawa2022expressions/","section":"publication","summary":"Proc. of the 31st ACM International Conference on Information \u0026 Knowledge Management (CIKM2022). (**Acceptance rate = 29.04%**)","tags":["International Conference","Refereed","Natural Language Processing","International Publication"],"title":"Expressions Causing Differences in Emotion Recognition in Social Networking Service Documents","type":"publication"},{"authors":["Tsubasa Nakagawa","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the ACM CIKM2022.\nTsubasa Nakagawa, Shunsuke Kitada, and Hitoshi Iyatomi. â€œExpressions Causing Differences in Emotion Recognition in Social Networking Service Documentsâ€ ","date":1659279600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e9a3647adb8bb382c82d31e86951b563","permalink":"https://shunk031.me/news/acceptance-to-cikm2022/","publishdate":"2022-08-01T00:00:00+09:00","relpermalink":"/news/acceptance-to-cikm2022/","section":"news","summary":"The following paper has been accepted to the ACM CIKM2022.\nTsubasa Nakagawa, Shunsuke Kitada, and Hitoshi Iyatomi. â€œExpressions Causing Differences in Emotion Recognition in Social Networking Service Documentsâ€ ","tags":["News"],"title":"Accepted our paper to ACM CIKM2022","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":"ã€NLP Hacks vol.6ã€‘NLP ã®ç ”ç©¶ã‚’åŠ é€Ÿã•ã›ã‚‹ AllenNLP å…¥é–€ å‹•ç”» - Video è³‡æ–™ - Slides ","date":1658485800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e957e1e81c20f7bc1f6c57d42c0f7a75","permalink":"https://shunk031.me/event/nlp-hacks-6th/","publishdate":"2022-07-22T20:30:00+09:00","relpermalink":"/event/nlp-hacks-6th/","section":"event","summary":"In this talk, I presented to introduce AllenNLP for accelerating NLP research in Japanese.","tags":["Lightning Talk","NLP","AllenNLP","Youtube","Invited Talk"],"title":"[Invited Talk] Introduction to AllenNLP to accelerate NLP research","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got JASSO Scholarship for Top 30% Excellent Graduate School Students (793,000 JPY) from Japan Student Services Organization.\n","date":1656601200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"05ce37ac8edaeb181dea74e945796ad9","permalink":"https://shunk031.me/news/jasso-scholarship-2022/","publishdate":"2022-07-01T00:00:00+09:00","relpermalink":"/news/jasso-scholarship-2022/","section":"news","summary":"Got JASSO Scholarship for Top 30% Excellent Graduate School Students (793,000 JPY) from Japan Student Services Organization.\n","tags":["News","Awards and Grants"],"title":"Got JASSO Scholarship for Top 30% Excellent Graduate School Students","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":"æ³•æ”¿å¤§å­¦å¤§å­¦é™¢å…¥å­¦æ¡ˆå†… 2023\nè©²å½“ãƒšãƒ¼ã‚¸ã®ã¿ (PDF, 1.7MBï¼‰ å…¨ä½“ (PDF, 61.8MBï¼‰ äººå·¥çŸ¥èƒ½ã¨äººãŒã‚ˆã‚Šã‚ˆãå…±å­˜ã™ã‚‹ãŸã‚ AI æŠ€è¡“ã®æ‹¡å¼µã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ åŒ—ç”°ä¿Šè¼” åšå£«å¾ŒæœŸèª²ç¨‹åœ¨å­¦ä¸­ / æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡\nç ”ç©¶ãƒ†ãƒ¼ãƒ æ‘‚å‹•ã«é ‘å¥ã§è§£é‡ˆå¯èƒ½ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨ãã®è§£é‡ˆæ€§ã®è©•ä¾¡\nç§ã®ç ”ç©¶ AI æŠ€è¡“ã®ä¸€ã¤ã€è‡ªç„¶è¨€èªå‡¦ç†ã«ã¤ã„ã¦ç ”ç©¶ã‚’é€²ã‚ã‚‹ä¸­ã§ã€ç‰¹ã«æ·±å±¤å­¦ç¿’ã¨å‘¼ã°ã‚Œã‚‹ã€äººé–“ã®è„³ã‚’æ¨¡å€£ã—ãŸæ‰‹æ³•ã‚’ãƒ™ãƒ¼ã‚¹ã«äººé–“ã®è¨€è‘‰ã‚’ç†è§£ã•ã›ã‚‹æ–¹æ³•ã«ã¤ã„ã¦è€ƒãˆã¦ã„ã¾ã™ã€‚æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã¯ã€äººé–“ã¨åŒç­‰ä»¥ä¸Šã®è¨€èªç†è§£æ€§èƒ½ã‚’ç¤ºã™äºˆæ¸¬ã‚’æç¤ºã§ãã‚‹ä¸€æ–¹ã§ã€ãªãœãã®äºˆæ¸¬ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã®ã‹ã‚’äººé–“ãŒç†è§£ã™ã‚‹ã®ã¯ç¾æ™‚ç‚¹ã§ã¯å›°é›£ã§ã™ã€‚äººå·¥çŸ¥èƒ½æŠ€è¡“ã¯æ¯æ—¥ã®ã‚ˆã†ã«æ•°å¤šãã®è«–æ–‡ãŒç™ºè¡¨ã•ã‚Œã€äººé–“ã‚’å‡Œé§•ã™ã‚‹ã‚ˆã†ãªçµæœã‚‚å‡ºå§‹ã‚ã¦ã„ã¾ã™ã€‚ãã†ã—ãŸä¸­ã€ç§ã¯ã“ã®ã‚ˆã†ãª AI æŠ€è¡“ãŒè‡ªèº«ã®äºˆæ¸¬ã‚’ã€Œèª¬æ˜å¯èƒ½ã€ã«ã—ã€äººé–“ãŒã—ã£ã‹ã‚Šã€Œè§£é‡ˆã€ã§ãã‚‹ã‚ˆã†ã«æŠ€è¡“ã‚’æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€AI æŠ€è¡“ã¨äººé–“ã®å…±å­˜ã‚’åŠ©ã‘ã‚‹ã‚ˆã†ãªå–ã‚Šçµ„ã¿ã‚’è¡Œã£ã¦çµæœã‚’å‡ºã—ã€ä¸–ç•Œã«ç™ºè¡¨ã—ãŸã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚\nå°‚æ”»ã®é­…åŠ› ç ”ç©¶æ´»å‹•ã¸ã®æ”¯æ´ä½“åˆ¶ãŒéå¸¸ã«æ‰‹åšãã€ç§ã‚‚ãã®ãŠã‹ã’ã§æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ã¨ã„ã†ã€ã‹ãªã‚Šã®ç‹­ãé–€ã§ã€ã‹ã¤è‹¥æ‰‹ç ”ç©¶è€…ã®ç™»ç«œé–€ã¨ã‚‚è¨€ã‚ã‚Œã‚‹åˆ¶åº¦ã«å¿œå‹Ÿã€æ¡ç”¨ã•ã‚Œã¾ã—ãŸã€‚ç‰¹åˆ¥ç ”ç©¶å“¡ã«æ¡ç”¨å¾Œã‚‚ã€å›½ã‹ã‚‰äº¤ä»˜ã•ã‚Œã‚‹ç§‘ç ”è²»ã®å–ã‚Šæ‰±ã„ã«ã¤ã„ã¦äº‹å‹™æ–¹ãŒã—ã£ã‹ã‚Šã‚µãƒãƒ¼ãƒˆã—ã¦ãã ã•ã‚‹ãŸã‚ã€ä½•ä¸è‡ªç”±ãªãç ”ç©¶ã«å°‚å¿µã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä»–ã®ç§ç«‹å¤§å­¦ã‚„å›½ç«‹å¤§å­¦ã«ã‚‚è² ã‘ãªã„ç´ æ™´ã‚‰ã—ã„å–ã‚Šçµ„ã¿ã ã¨æ„Ÿè¬ã—ã¦ã„ã¾ã™ã€‚\n","date":1652704222,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"616ae2e0fcde5425614d3b95e852cac5","permalink":"https://shunk031.me/post/hosei-university-graduate-school-admission-guide-2023/","publishdate":"2022-05-16T21:30:22+09:00","relpermalink":"/post/hosei-university-graduate-school-admission-guide-2023/","section":"post","summary":"https://saas.actibookone.com/?cNo=110942Â¶m=MV8zXzc=\u0026pNo=1","tags":["Interview","Hosei University"],"title":"æ³•æ”¿å¤§å­¦å¤§å­¦é™¢å…¥å­¦æ¡ˆå†… 2023 - å­¦ç”Ÿãƒ»ä¿®äº†ç”Ÿã®å£°","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"My comments as a representative of alumni were published in the admission guide of Hosei University. The comments can be found here.\n","date":1651330800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ec671b4dddb804f9483d2cdeaf112a1f","permalink":"https://shunk031.me/news/hosei-univeristy-graduate-school-admission-guide-2023/","publishdate":"2022-05-01T00:00:00+09:00","relpermalink":"/news/hosei-univeristy-graduate-school-admission-guide-2023/","section":"news","summary":"My comments as a representative of alumni were published in the admission guide of Hosei University. The comments can be found here.\n","tags":["News"],"title":"My comments were published in the admission guide of Hosei University","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi","Yoshifumi Seki"],"categories":["Computational Advertising"],"content":" arXiv MDPI SCImago ","date":1649000142,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"7a2ef492010bb4abc867bdcc6f5ce59b","permalink":"https://shunk031.me/publication/kitada2022ad/","publishdate":"2022-04-01T00:00:00+09:00","relpermalink":"/publication/kitada2022ad/","section":"publication","summary":"MDPI Appl. Sci. (**Impact Factor: 2.838** in 2021; [Top 10 ranking in Engineering \u0026 Computer Science (general) at  Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues\u0026hl=en\u0026vq=eng_enggeneral))","tags":["Journal","International Publication","Computational Advertising","Gunosy","Refereed","Open Access","MDPI"],"title":"Ad Creative Discontinuation Prediction with Multi-Modal Multi-Task Neural Survival Networks","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":" ","date":1648796100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"5add3aa75fa1982fd22e5160be0f9d92","permalink":"https://shunk031.me/event/jsps-dc-hosei-seminar-2023/","publishdate":"2022-04-01T00:00:00+09:00","relpermalink":"/event/jsps-dc-hosei-seminar-2023/","section":"event","summary":"In this seminar, I presented my experience as the only person who currently be adopted a JSPS DC2 in the campus.","tags":["Invited Talk","DC","JSPS"],"title":"[Invited Talk] My experience about JSPS Research Fellowship","type":"event"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi","Yoshifumi Seki"],"categories":["News"],"content":"The following paper has been accepted to the MDPI Applied Sciences.\nShunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. â€œAd Creative Discontinuation Prediction with Multi-Modal Multi-Task Neural Survival Networksâ€ ","date":1648479600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"48834926f9e90aaaaf058d273e0e72e2","permalink":"https://shunk031.me/news/acceptance-to-appl-sci-kitada2022ad/","publishdate":"2022-03-29T00:00:00+09:00","relpermalink":"/news/acceptance-to-appl-sci-kitada2022ad/","section":"news","summary":"The following paper has been accepted to the MDPI Applied Sciences.\nShunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. â€œAd Creative Discontinuation Prediction with Multi-Modal Multi-Task Neural Survival Networksâ€ ","tags":["News"],"title":"Accepted our journal paper to MDPI Applied Sciences journal","type":"news"},{"authors":["æ´¥å¶‹ ç¥ä»‹","é’æœ¨ åŒ ","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":"","date":1647481200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"35d62de7f53b55506d35c02ca6b8af49","permalink":"https://shunk031.me/publication/tsushima2022nlp/","publishdate":"2022-03-17T10:40:00+09:00","relpermalink":"/publication/tsushima2022nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 28 å›å¹´æ¬¡å¤§ä¼šï¼Œ2022.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2022"],"title":"ViT-CLT: ãƒ‘ãƒƒãƒåˆ†å‰²ã—ãŸæ–‡å­—ç”»åƒã‹ã‚‰åæ—å† è„šã‚’è€ƒæ…®ã—ãŸæ–‡æ›¸åˆ†é¡","type":"publication"},{"authors":["ä¸­å· ç¿¼","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Sentiment Analysis"],"content":"","date":1647481200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"fac2d4553b70d0afd50f1c15f4016559","permalink":"https://shunk031.me/publication/nakagawa2022nlp/","publishdate":"2022-03-17T10:40:00+09:00","relpermalink":"/publication/nakagawa2022nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 28 å›å¹´æ¬¡å¤§ä¼šï¼Œ2022.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2022"],"title":"æ›¸ãæ‰‹ã¨èª­ã¿æ‰‹ã®æ€’ã‚Šã®æ„Ÿæƒ…èªè­˜ã®å·®ã®åŸå› ã¨ãªã‚‹è¡¨ç¾ã®ç²å¾—","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Hosei University Research Grant for Doctoral Course Adopters in 2021 (480,000 JPY).\n","date":1638284400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d20ff134f961681ac316aaf0241f77da","permalink":"https://shunk031.me/news/hosei-university-research-grant-2021/","publishdate":"2021-12-01T00:00:00+09:00","relpermalink":"/news/hosei-university-research-grant-2021/","section":"news","summary":"Got Hosei University Research Grant for Doctoral Course Adopters in 2021 (480,000 JPY).\n","tags":["News","Awards and Grants"],"title":"Got Hosei University Research Grant 2021","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1638238933,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"65c3f99af1a3a2367dff367e2c1646c1","permalink":"https://shunk031.me/project/ceclcnn/","publishdate":"2021-11-30T11:22:13+09:00","relpermalink":"/project/ceclcnn/","section":"project","summary":"Official Implementation of [End-to-End Text Classification via Image-based Embedding using Character-level Networks](https://arxiv.org/abs/1810.03595)","tags":["PyTorch","Official Implementation","AllenNLP"],"title":"End-to-End Text Classification via Image-based Embedding using Character-level Networks","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":" ","date":1631785918,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"49bd4c35a40f289d9154a8c5a7f0d72e","permalink":"https://shunk031.me/post/cyberagent-ai-lab-research-internship-turning-point/","publishdate":"2021-09-16T18:51:58+09:00","relpermalink":"/post/cyberagent-ai-lab-research-internship-turning-point/","section":"post","summary":"https://www.cyberagent.co.jp/way/features/list/detail/id=26663","tags":["Interview","CyberAgent"],"title":"äººç”ŸãŒå¤‰ã‚ã‚‹ç ”ç©¶ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚·ãƒƒãƒ— ç¤¾ä¼šå®Ÿè£…ã«è§¦ã‚Œã¦åºƒãŒã‚‹ç ”ç©¶ã‚­ãƒ£ãƒªã‚¢ã¨ã¯","type":"post"},{"authors":["ä¸­å· ç¿¼","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Sentiment Analysis"],"content":"","date":1630580555,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"92afed904860fd8b6843bfe07c386776","permalink":"https://shunk031.me/publication/nakagawa2021yans/","publishdate":"2021-09-02T20:02:35+09:00","relpermalink":"/publication/nakagawa2021yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 16 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2021.","tags":["Domestic Conference","Non-refereed","YANS","YANS2021"],"title":"æ–‡ç« ã®æ›¸ãæ‰‹ãŒæŒã¤æ„Ÿæƒ…ã¨èª­ã¿æ‰‹ãŒå—ã‘ã‚‹æ„Ÿæƒ…ã®å·®ã«é–¢ã™ã‚‹åˆæœŸèª¿æŸ»ãŠã‚ˆã³è€ƒå¯Ÿ","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","å²©å´ ç¥è²´","å¯Œæ¨« é™¸","å±±å£ å…‰å¤ª","å½Œå†¨ ä»"],"categories":["Multi-modal Model","Computational Advertising"],"content":"","date":1630580217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"78ac82bd42eec12f0cb8a1292b6a981d","permalink":"https://shunk031.me/publication/kitada2021yans/","publishdate":"2021-09-02T19:56:57+09:00","relpermalink":"/publication/kitada2021yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 16 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2021.","tags":["Computational Advertising","Domestic Conference","Non-refereed","CyberAgent","YANS","YANS2021"],"title":"åŠ¹æœã®é«˜ã„ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ä½œæˆæ”¯æ´ã«å‘ã‘ãŸåºƒå‘Šãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®åŠ¹æœäºˆæ¸¬ãŠã‚ˆã³æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Attention Mechanisms","Adversarial Training","Text Classification","Question Answering","Natural Language Inference"],"content":"\n","date":1624924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"48bb6e3fb5028eeb5adf9b2a74511c16","permalink":"https://shunk031.me/publication/kitada2021attention/","publishdate":"2021-06-29T00:00:00Z","relpermalink":"/publication/kitada2021attention/","section":"publication","summary":"IEEE Access (**Impact Factor: 3.367** in 2020; [1st place in Engineering \u0026 Computer Science (general) at Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues\u0026hl=en\u0026vq=eng_enggeneral))","tags":["Journal","International Publication","Natural Language Processing","Refereed","Open Access","IEEE"],"title":"Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the IEEE Access.\nShunsuke Kitada and Hitoshi Iyatomi. â€œAttention Meets Perturbations: Robust and Interpretable Attention with Adversarial Trainingâ€ ","date":1624201200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e40b41a5d7dacfdfdf2aca1a98853257","permalink":"https://shunk031.me/news/acceptance-to-ieee-access-kitada2021attention/","publishdate":"2021-06-21T00:00:00+09:00","relpermalink":"/news/acceptance-to-ieee-access-kitada2021attention/","section":"news","summary":"The following paper has been accepted to the IEEE Access.\nShunsuke Kitada and Hitoshi Iyatomi. â€œAttention Meets Perturbations: Robust and Interpretable Attention with Adversarial Trainingâ€ ","tags":["News"],"title":"Accepted our paper to IEEE Access journal","type":"news"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»","é–¢ å–œå²"],"categories":["Computational Advertising","Probabilistic Model"],"content":"","date":1623224842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c2d42416208ff0501fe3a2725ef0783e","permalink":"https://shunk031.me/publication/kitada2021jsai/","publishdate":"2021-06-09T00:00:00+09:00","relpermalink":"/publication/kitada2021jsai/","section":"publication","summary":"äººå·¥çŸ¥èƒ½å­¦ä¼šç¬¬ 35 å›å…¨å›½å¤§ä¼šï¼Œ2021.","tags":["Domestic Conference","Non-refereed","Computational Advertising","Gunosy","JSAI","JSAI2021"],"title":"åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–è©•ä¾¡ã®ãŸã‚ã®æ·±å±¤ç¢ºç‡åŸ‹ã‚è¾¼ã¿ã®å­¦ç¿’","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Fellows, Grant Number 21J14143 (2021-2023, 1,500,000 JPY)\n","date":1619850561,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"3c99c40cbf9e6d7b57f88e644969e0f0","permalink":"https://shunk031.me/news/jsps-research-fellow-dc2/","publishdate":"2021-05-01T15:29:21+09:00","relpermalink":"/news/jsps-research-fellow-dc2/","section":"news","summary":"Got Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Fellows, Grant Number 21J14143 (2021-2023, 1,500,000 JPY)\n","tags":["News","Awards and Grants"],"title":"Got Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Fellows","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Attention Mechanisms","Text Classification","Question Answering","Natural Language Inference","Virtual Adversarial Training"],"content":"The manuscript accepted for publication in the Applied Intelligence journal can be found here.\n","date":1618907307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"636851b5f7f10be384a7ac7dcbcf93f4","permalink":"https://shunk031.me/publication/kitada2021making/","publishdate":"2021-04-20T17:28:27+09:00","relpermalink":"/publication/kitada2021making/","section":"publication","summary":"Preprint version of the Springer Applied Intelligence paper [\"Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training\"](/publication/kitada2022making)","tags":["Preprint"],"title":"Making Attention Mechanisms More Robust and Interpretable with Virtual Adversarial Training for Semi-Supervised Text Classification","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":" ","date":1617807600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"dd3a4e2e62fa3f08b752e62502728c50","permalink":"https://shunk031.me/post/jsps-dc2-applied-informatics/","publishdate":"2021-04-08T00:00:00+09:00","relpermalink":"/post/jsps-dc2-applied-informatics/","section":"post","summary":"https://ai.ws.hosei.ac.jp/wp/news/news20210408/","tags":["Interview","Hosei University","DC","JSPS"],"title":"åŒ—ç”°ä¿Šè¼”ã•ã‚“ (D2), æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ã«","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Adopted Japan Society for the Promotion of Science (JSPS) Research Fellowship for Young Scientists (DC2) (200,000 JPY / month).\n","date":1617202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"21fa98c201448f286c22d68903dce408","permalink":"https://shunk031.me/news/adopted-jsps-dc2/","publishdate":"2021-04-01T00:00:00+09:00","relpermalink":"/news/adopted-jsps-dc2/","section":"news","summary":"Adopted Japan Society for the Promotion of Science (JSPS) Research Fellowship for Young Scientists (DC2) (200,000 JPY / month).\n","tags":["News","Awards and Grants"],"title":"Adopted as a JSPS DC2","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"An article about my adoption of JSPS DC2 published on the website of the Department of Applied Informatics at Hosei University.\nSee also åŒ—ç”°ä¿Šè¼”ã•ã‚“ (D2), æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ã«. ","date":1617202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"58eb3be76f9d9dc8cfcf9d5f590ab7ef","permalink":"https://shunk031.me/news/jsps-dc2-applied-informatics/","publishdate":"2021-04-01T00:00:00+09:00","relpermalink":"/news/jsps-dc2-applied-informatics/","section":"news","summary":"An article about my adoption of JSPS DC2 published on the website of the Department of Applied Informatics at Hosei University.\nSee also åŒ—ç”°ä¿Šè¼”ã•ã‚“ (D2), æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ã«. ","tags":["News"],"title":"An article about my adoption of JSPS DC2 on the website of the Department of Applied Informatics at Hosei University","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":" ","date":1615974300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0e4f9d39cc238aea2820e5105e08aca0","permalink":"https://shunk031.me/event/univ-carthage-hosei/","publishdate":"2021-03-06T21:07:45+09:00","relpermalink":"/event/univ-carthage-hosei/","section":"event","summary":"This webinar introduces advanced research projects on intelligent robotics, Machine Learning and Distributed System under the joint auspices of the engineering faculties of University of Carthage and Hosei University, IIST: Institute of Integrated Science and Technology, to promote international research and educational collaboration among their affiliated schools.","tags":["Invited Talk","Invited Speaker"],"title":"[Invited Talk] Recent Issues in Intelligent Robotics, Machine Learning, and Distributed System","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":" ","date":1614870000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c7b9891cb955350691138b45f45eecd1","permalink":"https://shunk031.me/post/jsps-dc2-koganei/","publishdate":"2021-03-05T00:00:00+09:00","relpermalink":"/post/jsps-dc2-koganei/","section":"post","summary":"https://www.hosei.ac.jp/koganei/pickup/article-20210305121330/","tags":["Interview","Hosei University","DC","JSPS"],"title":"ç†å·¥å­¦ç ”ç©¶ç§‘ã®åŒ—ç”°ä¿Šè¼”æ°ãŒæ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ (DC2) ã«å†…å®š","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"An article about my adoption of JSPS DC2 published on the website of Hosei Univerisyt Graduate School.\nSee also ç†å·¥å­¦ç ”ç©¶ç§‘ã®åŒ—ç”°ä¿Šè¼”æ°ãŒæ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ (DC2) ã«å†…å®š. ","date":1614524400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"6ff8cf992f621e8dbfc34a111b2b8674","permalink":"https://shunk031.me/news/jsps-dc2-koganei/","publishdate":"2021-03-01T00:00:00+09:00","relpermalink":"/news/jsps-dc2-koganei/","section":"news","summary":"An article about my adoption of JSPS DC2 published on the website of Hosei Univerisyt Graduate School.\nSee also ç†å·¥å­¦ç ”ç©¶ç§‘ã®åŒ—ç”°ä¿Šè¼”æ°ãŒæ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ (DC2) ã«å†…å®š. ","tags":["News"],"title":"An article about my adoption of JSPS DC2 on the website of Hosei University Graduate School","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["DC"],"content":"2021 å¹´ 4 æœˆã‚ˆã‚Šæ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šã®ç‰¹åˆ¥ç ”ç©¶å“¡ DC2 ã«æ¡ç”¨ã—ã¦ã„ãŸã ãã¾ã—ãŸï¼ˆè£œæ¬ ç¹°ã‚Šä¸Šã’æ¡ç”¨ï¼‰ã€‚ å¼Šå­¦ã§ã¯æ®†ã©å­¦æŒ¯ç‰¹åˆ¥ç ”ç©¶å“¡ã®æ¡ç”¨å®Ÿç¸¾ãŒãªãèº«è¿‘ã«ç›¸è«‡ã§ãã‚‹æ–¹ã‚‚éå¸¸ã«å°‘ãªã‹ã£ãŸãŸã‚ 1ã€ ãã†ã—ãŸå¢ƒé‡ã®æ–¹ ï¼ˆã„ã‚ã‚†ã‚‹ãƒ“ãƒƒã‚¯ãƒ©ãƒœã§ã¯ãªã„æ–¹ã€æ—§å¸å¤§ãƒ»ä¸Šä½ç§ç«‹å¤§æ‰€å±ã§ã¯ãªã„æ–¹ï¼‰ ã«å°‘ã—ã§ã‚‚ãŠåŠ›ã«ãªã‚ŒãŸã‚‰ã¨æ€ã„ã€ç”³è«‹æ›¸ã®å…¬é–‹ã«è‡³ã‚Šã¾ã—ãŸã€‚2\nè³ªå•ç­‰ã«ã‚‚ã§ãã‚‹é™ã‚ŠãŠç­”ãˆã—ã¾ã™ã€‚é€£çµ¡å…ˆã¯ @shunk031 ã‚‚ã—ãã¯ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®ãƒšãƒ¼ã‚¸ ã‚’å‚ç…§ã—ã¦ä¸‹ã•ã„ã€‚\næ‘‚å‹•ã«é ‘å¥ã§è§£é‡ˆå¯èƒ½ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã¨ãã®è§£é‡ˆæ€§ã®è©•ä¾¡ ä»¤å’Œ 3 å¹´åº¦æ¡ç”¨åˆ† ç‰¹åˆ¥ç ”ç©¶å“¡ - DC ç”³è«‹æ›¸ (PDF, 1.8MB)\nå¯©æŸ»åŒºåˆ†ã¯ æƒ…å ±å­¦ ã® çŸ¥èƒ½æƒ…å ±å­¦é–¢é€£ ã§ã€å°‚é–€åˆ†é‡ã¯ è‡ªç„¶è¨€èªå‡¦ç† ã¨ã—ã¦ç”³è«‹ã—ã¾ã—ãŸã€‚ ç ”ç©¶ãƒ»è·æ­´ç­‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¿°ã—ã¾ã—ãŸã€‚ç‰¹ã« TA ã‚„ RAã€ä¼æ¥­ã§ã®ãƒªã‚µãƒ¼ãƒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç­‰ã®çµŒé¨“ã¯ç ”ç©¶é‚è¡Œèƒ½åŠ›ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨è€ƒãˆã¾ã—ãŸï¼š\n2016 å¹´ 10 æœˆ ï½ 2017 å¹´ 12 æœˆ NVIDIA Japan GTC Japan ãƒ†ã‚£ãƒ¼ãƒãƒ³ã‚°ãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ 2018 å¹´ 4 æœˆ ï½ 2020 å¹´ 3 æœˆ ãƒ†ã‚£ãƒ¼ãƒãƒ³ã‚°ãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ï¼ˆæ³•æ”¿å¤§å­¦ï¼‰ 2018 å¹´ 7 æœˆ ï½ 2020 å¹´ 8 æœˆ ï¼ˆäºˆå®šï¼‰æ ªå¼ä¼šç¤¾ Gunosy ãƒªã‚µãƒ¼ãƒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ 2019 å¹´ 6 æœˆ ï½ 2019 å¹´ 7 æœˆ ã‚¨ãƒ ã‚¹ãƒªãƒ¼æ ªå¼ä¼šç¤¾ R\u0026amp;D ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ 2020 å¹´ 4 æœˆ ï½ 2021 å¹´ 3 æœˆ ï¼ˆäºˆå®šï¼‰ãƒ†ã‚£ãƒ¼ãƒãƒ³ã‚°ãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆæ³•æ”¿å¤§å­¦ï¼‰ 2020 å¹´ 4 æœˆ ï½ 2021 å¹´ 3 æœˆ ï¼ˆäºˆå®šï¼‰ãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆæ³•æ”¿å¤§å­¦ï¼‰ ä»Šå›ã®é¸è€ƒçŠ¶æ³ã¯ç”³è«‹è€…æ•° ï¼ˆå–ã‚Šä¸‹ã’è€…æ•°å«ã‚€ï¼‰ 416 åã®ã†ã¡ã€1 æ¬¡æ¡ç”¨ãƒ» 2 æ¬¡æ¡ç”¨ãƒ»è£œæ¬ ãã‚Œãã‚Œä»¥ä¸‹ã®äººæ•°ã§ã€æ¡ç”¨ç‡ã¯ 20.2 % ç¨‹åº¦ã§ã—ãŸï¼ˆè£œæ¬ ç¹°ä¸ŠçµæœãŒé–‹ç¤ºã•ã‚ŒãŸ 2021 å¹´ 2 æœˆ 24 æ—¥ç¾åœ¨ 3ï¼‰ã€‚\næ¡ç”¨å†…å®šè€…æ•° 84 å 1 æ¬¡æ¡ç”¨å†…å®š 73 å 2 æ¬¡æ¡ç”¨å†…å®šå€™è£œè€…æ•° 19 å 2 æ¬¡æ¡ç”¨å†…å®š 8 å è£œæ¬ ç¹°ã‚Šä¸Šã’ 3 å è£œæ¬ ç¹°ã‚Šä¸Šã’æ¡ç”¨ã«é–¢ã—ã¦æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šã® è£œæ¬ æ¡ç”¨å†…å®šè€…ã®çŠ¶æ³ã«ã¤ã„ã¦ ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã‚‹ã¨ ä¾‹å¹´ DC2 å…¨ä½“ã§æ•°åç¨‹åº¦ã¨ã„ã†çŠ¶æ³ã§ã€ä»Šå›ã®ç¹°ã‚Šä¸Šã’æ¡ç”¨ã¯éå¸¸ã«ç¨€ãªã‚±ãƒ¼ã‚¹ã§ã—ãŸã€‚4\nåŸ·ç­†ã«ã‚ãŸã‚Šå‚è€ƒã«ã—ãŸã‚‚ã® æƒ…å ±å­¦ãƒ»è‡ªç„¶è¨€èªå‡¦ç†ç­‰ã®åˆ†é‡ã§æ´»èºã•ã‚Œã¦ã„ã‚‹æ–¹ã€…ã®ãƒšãƒ¼ã‚¸ã‚„ç”³è«‹æ›¸ã‚’å‚è€ƒã«ã—ã¾ã—ãŸï¼š\nå°ç”ºå…ˆç”Ÿã® DC1, DC2 ç”³è«‹æ›¸ ï¼ˆè¨€èªå­¦ï¼Œå¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åè©å¥ã®é …æ§‹é€ è§£æï¼‰ å‰ç”°å…ˆç”Ÿã® DC1 ç”³è«‹æ›¸ ï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±å­¦ï¼Œæ™‚ç©ºé–“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«çµ±åˆã—ãŸã‚¦ã‚§ãƒ–æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿç¾ï¼‰ å°ç”°ã•ã‚“ã® DC1 ç”³è«‹æ›¸ ï¼ˆçŸ¥èƒ½æƒ…å ±å­¦ï¼ŒåŒæ™‚éŸ³å£°ç¿»è¨³æŠ€è¡“ã®ç²¾åº¦å‘ä¸Šï¼‰ ã€å­¦æŒ¯ç”³è«‹æ›¸ã®æ›¸ãæ–¹ã¨ã‚³ãƒ„ ï¼¤ï¼£ï¼ï¼°ï¼¤ç²å¾—ã‚’ç›®æŒ‡ã™è‹¥è€…ã¸ã€ï¼ˆå¤§ä¸Š é›…å²ï¼‰ï½œè¬›è«‡ç¤¾ BOOK å€¶æ¥½éƒ¨ ç§ã¯ç”³è«‹æ›¸åŸ·ç­†æ™‚ç‚¹ã§æœ€é›£é–¢å›½éš›ä¼šè­°ã® 1 ã¤ã§ã‚ã‚‹å­¦ä¼šã«è«–æ–‡æ¡æŠçµŒé¨“ãŒã‚ã£ãŸã®ã§ã€ç‰¹ã«å°ç”°ã•ã‚“ã®ç”³è«‹æ›¸ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å±•é–‹ã‚„æœ€é›£é–¢å›½éš›ä¼šè­°ã¸ã®è«–æ–‡æ¡æŠçµŒé¨“ã®å¼·èª¿ã®ä»•æ–¹ç­‰ã‚’å‚è€ƒã«ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚\nå­¦æŒ¯ DC2 æ¡ç”¨ã®ä½“é¨“è«‡ã«ã¤ã„ã¦ï¼ˆ2022/04/01 æ›´æ–°ï¼‰ å­¦å†…ã§é–‹å‚¬ã•ã‚ŒãŸ 2023ï¼ˆä»¤å’Œ 5ï¼‰å¹´åº¦ æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ï¼ˆï¼¤ï¼£ï¼‘ãƒ»ï¼¤ï¼£ï¼’ï¼‰ç”³è«‹ã«é–¢ã™ã‚‹ ã‚»ãƒŸãƒŠãƒ¼ ã«ã¦ã€ç§ã®å­¦æŒ¯ DC2 ä½“é¨“è«‡ï¼ˆç™ºè¡¨è³‡æ–™ ) ã¨ã„ã†è¬›æ¼”ã‚’ã—ã¾ã—ãŸã€‚å­¦æŒ¯ç‰¹åˆ¥ç ”ç©¶å“¡ã«ãªã£ã¦ã‚ˆã‹ã£ãŸã“ã¨ã€æ¡ç”¨ã¾ã§ã®ï¼ˆé•·ã‹ã£ãŸï¼‰é“ã®ã‚Šã€ç”³è«‹ä½œä½œæˆã®æ³¨æ„ç‚¹ãªã©ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚\nå­¦å†…ã§ ç”³è«‹ã‚»ãƒŸãƒŠãƒ¼ã®é–‹å‚¬ ã‚„ DC2 ç”³è«‹æ”¯æ´åˆ¶åº¦ ãŒã‚ã‚Šã€ãã‚Œãã‚Œæ´»ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚Â â†©ï¸\nå¼Šå­¦ã‹ã‚‰ç‰¹åˆ¥ç ”ç©¶å“¡ã«æ¡ç”¨ã•ã‚ŒãŸã®ãŒéå¸¸ã«ç¨€ãªäº‹ä¾‹ã ã£ãŸã‚ˆã†ã§ã€å¤§å­¦åºƒå ± ã‚„ å­¦ç§‘ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ ã§å–ã‚Šä¸Šã’ã¦ã„ãŸã ã‘ã¾ã—ãŸã€‚Â â†©ï¸\nä»¤å’Œ 3 å¹´åº¦ï¼ˆ2021 å¹´åº¦ï¼‰ã® æ¡ç”¨çŠ¶æ³ ã§ã¯ã€æ¡ç”¨å†…å®šè€…æ•°ã¯ 83 äººã§ã—ãŸã€‚æœ€çµ‚çš„ãªæ¡æŠç‡ã¯ 19.8 % ç¨‹åº¦ã§ã—ãŸã€‚Â â†©ï¸\næœ€çµ‚çš„ãªé¸è€ƒçµæœãŒå‡ºãªã„ã¨ã‚ã‹ã‚Šã¾ã›ã‚“ãŒã€ã‚³ãƒ­ãƒŠç¦ã«ã‚ˆã‚‹åšå£«å­¦ç”Ÿæ”¯æ´ã®é¢¨æ½®ã«ã‚ˆã‚Š 2021 å¹´åº¦ã®ç¹°ã‚Šä¸Šã’æ¡ç”¨ãŒå…¨ä½“çš„ã«å¢—ãˆã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ ä»¤å’Œ 3 å¹´åº¦ï¼ˆ2021 å¹´åº¦ï¼‰ã® è£œæ¬ æ¡ç”¨å†…å®šè€…ã®çŠ¶æ³ã«ã¤ã„ã¦ ã§ã¯ã€è£œæ¬ æ¡ç”¨å†…å®šè€…ã¯ 44 äººã§ã—ãŸã€‚å‰å¹´åº¦ã® 8 äººã«æ¯”ã¹ã¦ 36 äººã»ã©å¢—ãˆã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚Â â†©ï¸\n","date":1614427822,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"af2f28a271500480320a8e09cf191c09","permalink":"https://shunk031.me/post/dc2/","publishdate":"2021-02-27T21:10:22+09:00","relpermalink":"/post/dc2/","section":"post","summary":"è£œæ¬ ç¹°ã‚Šä¸Šã’ã§ 2021 å¹´åº¦å­¦æŒ¯ç‰¹åˆ¥ç ”ç©¶å“¡ DC2 ã«æ¡ç”¨ã—ã¦ã„ãŸã ã„ãŸéš›ã®ç”³è«‹æ›¸ã§ã™ã€‚","tags":["DC","JSPS","Hosei University"],"title":"æ—¥æœ¬å­¦è¡“æŒ¯èˆˆä¼šç‰¹åˆ¥ç ”ç©¶å“¡ (DC2) ç”³è«‹æ›¸ãƒ»å­¦æŒ¯","type":"post"},{"authors":["å¤§ç•‘ å’Œä¹Ÿ","é•·æ¾¤ é§¿å¤ª","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Image Caption"],"content":"","date":1610699347,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"28ce38c68c0865c1eebe34ea4fd9f490","permalink":"https://shunk031.me/publication/ohata2021nlp/","publishdate":"2021-03-15T00:00:00+09:00","relpermalink":"/publication/ohata2021nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 27 å›å¹´æ¬¡å¤§ä¼šï¼Œ2021.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2021"],"title":"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆæ™‚ä½å“è³ªãƒ‡ãƒ¼ã‚¿äº‹å‰æ¤œçŸ¥ã®è©¦ã¿","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Virtual Adversarial Training"],"content":"","date":1610699320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"efdb12800c4b3dc873b238902c067396","permalink":"https://shunk031.me/publication/kitada2021nlp/","publishdate":"2021-03-15T00:00:00+09:00","relpermalink":"/publication/kitada2021nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 27 å›å¹´æ¬¡å¤§ä¼šï¼Œ2021.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2021"],"title":"åŠæ•™å¸«ã‚ã‚Šæ–‡æ›¸åˆ†é¡ã®ãŸã‚ã®ä»®æƒ³æ•µå¯¾çš„å­¦ç¿’ã«ã‚ˆã‚‹æ³¨æ„æ©Ÿæ§‹ã®é ‘å¥æ€§ãŠã‚ˆã³è§£é‡ˆæ€§ã®å‘ä¸Š","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Hosei University Research Grant for Doctoral Course Adopters in 2020 (480,000 JPY).\n","date":1606748400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"82fb62448425f6845f5c139e1ab9ddea","permalink":"https://shunk031.me/news/hosei-university-research-grant-2020/","publishdate":"2020-12-01T00:00:00+09:00","relpermalink":"/news/hosei-university-research-grant-2020/","section":"news","summary":"Got Hosei University Research Grant for Doctoral Course Adopters in 2020 (480,000 JPY).\n","tags":["News","Awards and Grants"],"title":"Got Hosei University Research Grant 2020","type":"news"},{"authors":["Takumi Aoki","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":" ","date":1603606541,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c24baeac4615f0b24d2a70fa89ba9351","permalink":"https://shunk031.me/publication/aoki2020text/","publishdate":"2020-10-25T15:15:41+09:00","relpermalink":"/publication/aoki2020text/","section":"publication","summary":"Proc. of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop (AACL-IJCNLP SRW 2020)","tags":["International Conference","Refereed","Natural Language Processing","International Publication","AACL-IJCNLP","AACL-IJCNLP2020"],"title":"Text Classification through Glyph-aware Disentangled Character Embedding and Semantic Sub-character Augmentation","type":"publication"},{"authors":["Takumi Aoki","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the AACL-IJCNLP 2020 Student Research Workshop (SRW).\nTakumi Aoki, Shunsuke Kitada, and Hitoshi Iyatomi. â€œText Classification through Glyph-aware Disentangled Character Embedding and Semantic Sub-character Augmentationâ€ ","date":1603465200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d0d66a9ed74d02dd5a6abd09a6fb91a9","permalink":"https://shunk031.me/news/acceptance-to-aacl2020srw/","publishdate":"2020-10-24T00:00:00+09:00","relpermalink":"/news/acceptance-to-aacl2020srw/","section":"news","summary":"The following paper has been accepted to the AACL-IJCNLP 2020 Student Research Workshop (SRW).\nTakumi Aoki, Shunsuke Kitada, and Hitoshi Iyatomi. â€œText Classification through Glyph-aware Disentangled Character Embedding and Semantic Sub-character Augmentationâ€ ","tags":["News"],"title":"Accepted our paper to AACL-IJCNLP2020 SRW","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1601834172,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2b2c08b4f4adcafc2f06d3132cd24df2","permalink":"https://shunk031.me/project/allennlp-eraser/","publishdate":"2020-10-05T02:56:12+09:00","relpermalink":"/project/allennlp-eraser/","section":"project","summary":"Collection of AllenNLP DatasetReaders for ERASER","tags":["PyTorch","AllenNLP"],"title":"AllenNLP Eraser","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1601564467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d23e2b7b87d2b9a22b5df166a4243589","permalink":"https://shunk031.me/project/attention-meets-perturbation/","publishdate":"2020-10-02T00:01:07+09:00","relpermalink":"/project/attention-meets-perturbation/","section":"project","summary":"Official Implementation of the paper [Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training](https://arxiv.org/abs/2009.12064)","tags":["PyTorch","Official Implementation","AllenNLP"],"title":"Attention Meets Perturbation","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Youtube"],"content":"ã‚ªãƒ¼ãƒ—ãƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ã«éš›ã—ã¦ã€å¿œç”¨æƒ…å ±å·¥å­¦ç§‘ã®å­¦ç§‘ä»£è¡¨ã¨ã—ã¦æ‰€å±ç ”ç©¶å®¤ã®ç´¹ä»‹å‹•ç”»ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚ è©³ç´°ã¯ æ³•æ”¿å¤§å­¦ ç†ç³» 3 å­¦éƒ¨åˆåŒ ã‚ªãƒ¼ãƒ—ãƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ | æ³•æ”¿å¤§å­¦ å…¥è©¦æƒ…å ±ã‚µã‚¤ãƒˆ ã‚’ã”è¦§ãã ã•ã„ã€‚\n","date":1598194800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"760d57c0c1500e4338aaddd86ae93047","permalink":"https://shunk031.me/post/lab-introduction-video-for-open-campus-2020/","publishdate":"2020-08-24T00:00:00+09:00","relpermalink":"/post/lab-introduction-video-for-open-campus-2020/","section":"post","summary":"2020 å¹´åº¦ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ã«éš›ã—ã¦ã€å¿œç”¨æƒ…å ±å·¥å­¦ç§‘ã®å­¦ç§‘ä»£è¡¨ã¨ã—ã¦æ‰€å±ç ”ç©¶å®¤ã®ç´¹ä»‹å‹•ç”»ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚","tags":["Hosei University","Youtube"],"title":"ç†å·¥å­¦éƒ¨å¿œç”¨æƒ…å ±å·¥å­¦ç§‘ã€€å½Œå†¨ä»å…ˆç”Ÿã€Œç§ãŸã¡ã®ç”Ÿæ´»ã«è²¢çŒ®ã™ã‚‹æ–°ã—ã„äººå·¥çŸ¥èƒ½æŠ€è¡“ã€","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1597670586,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"4ed4e4143ff239bb89386bfc48212feb","permalink":"https://shunk031.me/project/human-attention-map-for-text-classification/","publishdate":"2020-08-17T22:23:06+09:00","relpermalink":"/project/human-attention-map-for-text-classification/","section":"project","summary":"Reimplementation of the paper [Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words? (ACL2020)](https://aclanthology.org/2020.acl-main.419) in PyTorch","tags":["PyTorch","AllenNLP"],"title":"Human Attention Map for Text Classification","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got JASSO Scholarship for Top 10% Excellent Graduate School Students from Japan Student Services Organization.\n","date":1593529200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"8dce0aaf27b7c251d33fc639977fecde","permalink":"https://shunk031.me/news/jasso-scholarship-2020/","publishdate":"2020-07-01T00:00:00+09:00","relpermalink":"/news/jasso-scholarship-2020/","section":"news","summary":"Got JASSO Scholarship for Top 10% Excellent Graduate School Students from Japan Student Services Organization.\n","tags":["News","Awards and Grants"],"title":"Got JASSO Scholarship for Top 10% Excellent Graduate School Students","type":"news"},{"authors":["Mahmoud Daif","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","NLP for Arabic","Image-based Character Embedding"],"content":" ","date":1592743113,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2551580ffd2bab27a4b0f510d6badc9e","permalink":"https://shunk031.me/publication/daif2020aradic/","publishdate":"2020-06-21T21:38:33+09:00","relpermalink":"/publication/daif2020aradic/","section":"publication","summary":"Proc. of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop (ACL SRW 2020)","tags":["International Publication","Refereed","Natural Language Processing","ACL","ACL2020"],"title":"AraDIC: Arabic Document Classification using Image-Based Character Embeddings and Class-Balanced Loss","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»","é–¢ å–œå²"],"categories":["Computational Advertising","Survival Analysis"],"content":"","date":1592370417,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"3326717967b57db2a25a9f9986c0ff65","permalink":"https://shunk031.me/publication/kitada2020jsai/","publishdate":"2020-06-09T00:00:00+09:00","relpermalink":"/publication/kitada2020jsai/","section":"publication","summary":"äººå·¥çŸ¥èƒ½å­¦ä¼šç¬¬ 34 å›å¹´æ¬¡å¤§ä¼šï¼Œ2020.","tags":["Domestic Conference","Non-refereed","Computational Advertising","Gunosy","JSAI","JSAI2020"],"title":"ç”Ÿå­˜æ™‚é–“åˆ†æã‚’ç”¨ã„ãŸåºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ã®åœæ­¢äºˆæ¸¬","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":" ","date":1591681200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"459f353e108b1e89754fb6ca6235c570","permalink":"https://shunk031.me/event/jsai2020-os-invited-talk/","publishdate":"2020-04-12T12:37:54+09:00","relpermalink":"/event/jsai2020-os-invited-talk/","section":"event","summary":"In this talk, I will introduce my research topic: the framework of ad creative evaluation based on CVR prediction to support the creation of effective ad creative. Also, I will introduce several research topics related to the latest ad technology and discuss the current status and prospects achieved by the research on ad creative and machine learning techniques.","tags":["Invited Talk"],"title":"[Invited Talk] The Present and Future of Machine Learning for Ad Creatives","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News"],"content":"Invited talk on Organized Session in The 34th Annual Conference of the Japanese Society for Artificial Intelligence.\nSee also The Present and Future of Machine Learning for Ad Creatives ","date":1591628400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"da21bfd566e63d5cd3c19ebcbfb5bfa4","permalink":"https://shunk031.me/news/invited-talk-at-jsai-2020/","publishdate":"2020-06-09T00:00:00+09:00","relpermalink":"/news/invited-talk-at-jsai-2020/","section":"news","summary":"Invited talk on Organized Session in The 34th Annual Conference of the Japanese Society for Artificial Intelligence.\nSee also The Present and Future of Machine Learning for Ad Creatives ","tags":["News"],"title":"Invited Talk at JSAI 2020","type":"news"},{"authors":["Mahmoud Daif","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the ACL2020 Student Research Workshop (SRW).\nMahmoud Daif, Shunsuke Kitada, Hitoshi Iyatomi. â€œAraDIC: Arabic Document Classification using Image-Based Character Embeddings and Class-Balanced Lossâ€ ","date":1587135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"25a8884c97ae94882ff0b1b6ac242cb6","permalink":"https://shunk031.me/news/acceptance-to-acl2020srw/","publishdate":"2020-04-18T00:00:00+09:00","relpermalink":"/news/acceptance-to-acl2020srw/","section":"news","summary":"The following paper has been accepted to the ACL2020 Student Research Workshop (SRW).\nMahmoud Daif, Shunsuke Kitada, Hitoshi Iyatomi. â€œAraDIC: Arabic Document Classification using Image-Based Character Embeddings and Class-Balanced Lossâ€ ","tags":["News"],"title":"Accepted our paper to ACL2020 SRW","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Survey"],"content":"","date":1585657224,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e179a548101d564f3255c47c8816c328","permalink":"https://shunk031.me/project/paper-survey/","publishdate":"2020-03-31T21:20:24+09:00","relpermalink":"/project/paper-survey/","section":"project","summary":"ğŸ“šSurvey of previous research and related works on machine learning (especially Deep Learning) in Japanese.","tags":["Survey"],"title":"Paper Survey","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["HP"],"content":"","date":1585653624,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"04e1b8adf0c3aa05cf7af28a80ab7c9e","permalink":"https://shunk031.me/project/hosei-university-soft-tennis-club-hp/","publishdate":"2020-03-31T20:20:24+09:00","relpermalink":"/project/hosei-university-soft-tennis-club-hp/","section":"project","summary":":tennis: Hosei University Soft Tennis Club's home page repository.","tags":["HP"],"title":"Hosei University Soft Tennis Club Hp","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585554326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"b78711e47ff7a856ded486dd41bc6041","permalink":"https://shunk031.me/project/chainer-focalloss/","publishdate":"2020-03-30T16:45:26+09:00","relpermalink":"/project/chainer-focalloss/","section":"project","summary":"Implementation of [Focal loss](https://arxiv.org/abs/1708.02002) in Chainer.","tags":["Chainer"],"title":"chainer-FocalLoss","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585554211,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c61a0b0b9c9ba7f25c7bfafd488a4abd","permalink":"https://shunk031.me/project/chainer-ricap/","publishdate":"2020-03-30T16:43:31+09:00","relpermalink":"/project/chainer-ricap/","section":"project","summary":"Implementation of [RICAP](https://proceedings.mlr.press/v95/takahashi18a.html) in Chainer.","tags":["Chainer"],"title":"chainer-RICAP","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585553871,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a136c25a18bdfde39d7df2b28d55b229","permalink":"https://shunk031.me/project/lsuv-pytorch/","publishdate":"2020-03-30T16:37:51+09:00","relpermalink":"/project/lsuv-pytorch/","section":"project","summary":"Implementation of [LSUV](https://arxiv.org/abs/1511.06422) (Layer-sequential unit-variance) in PyTorch.","tags":["PyTorch"],"title":"LSUV.pytorch","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["PyTorch","Golang","libtorch","gin","api-server","cgo","docker"],"content":"","date":1585553722,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"9070912ad3be18046d320c1100e3b27b","permalink":"https://shunk031.me/project/libtorch-gin-api-server/","publishdate":"2020-03-30T16:35:22+09:00","relpermalink":"/project/libtorch-gin-api-server/","section":"project","summary":"High-speed Deep learning API Server with Libtorch (C++) and Gin (Golang)","tags":["PyTorch","Server"],"title":"libtorch-gin-api-server","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Tool"],"content":"","date":1585553601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"6d31b113ed2ea7d159d9c8a43a3720a2","permalink":"https://shunk031.me/project/nvhtop/","publishdate":"2020-03-30T16:33:21+09:00","relpermalink":"/project/nvhtop/","section":"project","summary":"A tool for enriching the output of nvidia-smi forked from peci1/nvidia-htop.","tags":["Tool"],"title":"nvhtop","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585494786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ea4db63c2dd419a67f9fad2771a2ba5a","permalink":"https://shunk031.me/project/chainer-meanteachers/","publishdate":"2020-03-30T00:13:06+09:00","relpermalink":"/project/chainer-meanteachers/","section":"project","summary":"Implementation of [Mean teachers are better role models](https://arxiv.org/abs/1703.01780) in Chainer.","tags":["Chainer"],"title":"chainer-MeanTeachers","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585494014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"9e47d57a246192269e40bbf7ed95393c","permalink":"https://shunk031.me/project/chainer-pyramidnet/","publishdate":"2020-03-30T00:00:14+09:00","relpermalink":"/project/chainer-pyramidnet/","section":"project","summary":"Implementation of [PyramidNet](https://arxiv.org/abs/1610.02915) in Chainer.","tags":["Chainer"],"title":"chainer-PyramidNet","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585493878,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"23a609072a621cf06a378bf9963f713c","permalink":"https://shunk031.me/project/chainer-inceptionresnetv2/","publishdate":"2020-03-29T23:57:58+09:00","relpermalink":"/project/chainer-inceptionresnetv2/","section":"project","summary":"Implementation of [InceptionResNetV2](https://arxiv.org/abs/1602.07261) in Chainer.","tags":["Chainer"],"title":"chainer-InceptionResNetV2","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585493747,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"d20ca1c133ea542fe62e525b6a1978be","permalink":"https://shunk031.me/project/chainer-imsat/","publishdate":"2020-03-29T23:55:47+09:00","relpermalink":"/project/chainer-imsat/","section":"project","summary":"Implementation of [IMSAT](https://arxiv.org/abs/1702.08720) in Chainer.","tags":["Chainer"],"title":"chainer-IMSAT","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1585420002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"960e0b85d8885513b5a6ec5918b7bcf2","permalink":"https://shunk031.me/project/chainer-center-loss/","publishdate":"2020-03-29T03:26:42+09:00","relpermalink":"/project/chainer-center-loss/","section":"project","summary":"Implementation of [Center Loss](https://link.springer.com/chapter/10.1007/978-3-319-46478-7_31) in Chainer.","tags":["Chainer"],"title":"chainer-center-loss","type":"project"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Interpretability","Adversarial Training"],"content":"","date":1584543600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"1445af435c5e24b7eb752fd9cf25461d","permalink":"https://shunk031.me/publication/kitada2020nlp/","publishdate":"2020-03-19T00:00:00+09:00","relpermalink":"/publication/kitada2020nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 26 å›å¹´æ¬¡å¤§ä¼šï¼Œ2020.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2020"],"title":"è§£é‡ˆå¯èƒ½ãªæ•µå¯¾çš„æ‘‚å‹•ã‚’ç”¨ã„ãŸé ‘å¥ãªæ³¨æ„æ©Ÿæ§‹ã®å­¦ç¿’","type":"publication"},{"authors":["é’æœ¨ åŒ ","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":"","date":1584457200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"04166a26cd938ced04e7fa1a6f6358a2","permalink":"https://shunk031.me/publication/aoki2020nlp/","publishdate":"2020-03-18T00:00:00+09:00","relpermalink":"/publication/aoki2020nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 26 å›å¹´æ¬¡å¤§ä¼šï¼Œ2020.","tags":["Domestic Conference","Non-refereed","ANLP","NLP2020"],"title":"æ–‡å­—å˜ä½ã®è§£é‡ˆå¯èƒ½ãªæ½œåœ¨è¡¨ç¾ã® data augmentation","type":"publication"},{"authors":["Mahmoud Daif","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Image-based Character Embedding","NLP for Arabic"],"content":"","date":1584370800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"f1c1b5437abbdeea44147ea0b2c87ca9","permalink":"https://shunk031.me/publication/daif2020nlp/","publishdate":"2020-03-17T00:00:00+09:00","relpermalink":"/publication/daif2020nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 26 å›å¹´æ¬¡å¤§ä¼šï¼Œ2020.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2020"],"title":"Image-based Character Embedding for Arabic Document Classification","type":"publication"},{"authors":["é•·æ¾¤ é§¿å¤ª","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":"","date":1584370800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"b4e1bdee44081fc83016404bdc3bb8f3","permalink":"https://shunk031.me/publication/nagasawa2020nlp/","publishdate":"2020-03-17T00:00:00+09:00","relpermalink":"/publication/nagasawa2020nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 26 å›å¹´æ¬¡å¤§ä¼šï¼Œ2020.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2020"],"title":"Script-aware embedding ã‚’ç”¨ã„ãŸæ–‡å­—è¡¨ç¾ã®ç²å¾—","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Interview"],"content":"æ—¥åˆŠå·¥æ¥­æ–°è é›»å­ç‰ˆ ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ç ”ç©¶ã€ã‚°ãƒã‚·ãƒ¼ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³é™¢ç”Ÿä¸»åŠ›ã« | æ—¥åˆŠå·¥æ¥­æ–°è é›»å­ç‰ˆ https://www.nikkan.co.jp/articles/view/00546182\nãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¤ãƒƒãƒ by æ—¥åˆŠå·¥æ¥­æ–°è ã‚°ãƒã‚·ãƒ¼ã€ã€Œç ”ç©¶é–‹ç™ºæ‹…å½“ã®ä¸»åŠ›ã«å¤§å­¦é™¢ç”Ÿã€ã§å¾—ãŸåŠ¹æœï½œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¤ãƒƒãƒ by æ—¥åˆŠå·¥æ¥­æ–°èç¤¾ https://newswitch.jp/p/20932\n","date":1580328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"5c3a28a3caefef2fef7a728e793e9e92","permalink":"https://shunk031.me/post/gunosy-intern-kdd-2019/","publishdate":"2020-01-30T05:00:00+09:00","relpermalink":"/post/gunosy-intern-kdd-2019/","section":"post","summary":"https://www.nikkan.co.jp/articles/view/00546182 and https://newswitch.jp/p/20932","tags":["Interview","Gunosy","Hosei University"],"title":"ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ç ”ç©¶ã€ã‚°ãƒã‚·ãƒ¼ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³é™¢ç”Ÿä¸»åŠ›ã«","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Interviewed by Nikkan Kogyo Shimbun, Ltd ( the article).\n","date":1580310000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"493f13827facad6372e759eadb0884de","permalink":"https://shunk031.me/news/interview-by-nikkan-kogyo-shinbun-2019/","publishdate":"2020-01-30T00:00:00+09:00","relpermalink":"/news/interview-by-nikkan-kogyo-shinbun-2019/","section":"news","summary":"Interviewed by Nikkan Kogyo Shimbun, Ltd ( the article).\n","tags":["News","Awards and Grants"],"title":"Interviewed by Nikkan Kogyo Shimbun, Ltd.","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Yoshifumi Seki"],"categories":null,"content":" ","date":1569566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"cca49157d7085ceedfb695ff170ed36a","permalink":"https://shunk031.me/event/text-analytics-symposium-2019/","publishdate":"2020-03-29T03:28:55+09:00","relpermalink":"/event/text-analytics-symposium-2019/","section":"event","summary":"This talk introduces a case study of research and development internship at Gunosy Inc.","tags":["Invited Talk"],"title":"[Invited Talk] ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒ“ã‚¹äº‹æ¥­è€…ã«ãŠã‘ã‚‹ç ”ç©¶é–‹ç™ºã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ ï½Gunosyæ ªå¼ä¼šç¤¾ï½","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":[],"content":" ","date":1568905200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ca39b50838c34c2e58a61976b785581d","permalink":"https://shunk031.me/post/award-yans-2019/","publishdate":"2019-09-20T00:00:00+09:00","relpermalink":"/post/award-yans-2019/","section":"post","summary":"https://www.hosei.ac.jp/gs/NEWS/zaigaku/koganei/20190920","tags":["Award","Hosei University"],"title":"ç†å·¥å­¦ç ”ç©¶ç§‘åœ¨å­¦ç”Ÿã®åŒ—ç”°ä¿Šè¼”æ°ãŒã€Œå¥¨åŠ±è³ã€ã‚’å—è³","type":"post"},{"authors":["Shunsuke Kitada, Ph.D.","Kaito Odagiri"],"categories":null,"content":"","date":1568516400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"f90d2319ca6b68eb17ac038a5b5667bc","permalink":"https://shunk031.me/event/hosei-scitech-forum-2019/","publishdate":"2019-09-16T00:00:00+09:00","relpermalink":"/event/hosei-scitech-forum-2019/","section":"event","summary":"ç†å·¥å­¦éƒ¨ å¿œç”¨æƒ…å ±å·¥å­¦ç§‘ çŸ¥çš„æƒ…å ±å‡¦ç†ç ”ç©¶å®¤ ã®ç ”ç©¶å®¤ç´¹ä»‹ã§ã™","tags":["Presentation"],"title":"[Invited Poster] Hosei Scitech Forum 2019","type":"event"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["News","Awards and Grants"],"content":"Got Honorable Mention in YANS 2019.\nã€å¥¨åŠ±è³ã€‘2/3\nãƒ»ã€Œè§£é‡ˆæ€§å‘ä¸Šã®ãŸã‚ã®æ³¨æ„æ©Ÿæ§‹ã¨æå¤±å‹¾é…ã«å¯¾ã™ã‚‹é–¢é€£æå¤±ã®å°å…¥ã€åŒ—ç”°ä¿Šè¼”, å½Œå†¨ä»\nãƒ»ã€Œæ–‡æ³•èª¤ã‚Šè¨‚æ­£ã‚’æ‹¡å¼µã—ãŸæ–°ã‚¿ã‚¹ã‚¯ã®ææ¡ˆã€ä¸‰ç”°é›…äºº, è©åŸæ­£äºº, å‚å£æ…¶ç¥, æ°´æœ¬æ™ºä¹Ÿ, éˆ´æœ¨æ½¤, ä¹¾å¥å¤ªéƒ#yans2019\nâ€” YANS (@yans_official) August 28, 2019 ","date":1566918000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"c42e454013468d5ceb9605484fcaccff","permalink":"https://shunk031.me/news/honorable-mention-in-yans-2019/","publishdate":"2019-08-28T00:00:00+09:00","relpermalink":"/news/honorable-mention-in-yans-2019/","section":"news","summary":"è§£é‡ˆæ€§å‘ä¸Šã®ãŸã‚ã®æ³¨æ„æ©Ÿæ§‹ã¨æå¤±å‹¾é…ã«å¯¾ã™ã‚‹é–¢é€£æå¤±ã®å°å…¥","tags":["News","Awards and Grants"],"title":"Honorable Mention in YANS 2019","type":"news"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Interpretability","Text Classification","YANS"],"content":"","date":1566831600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a965ab4099f7b9031cb682f8ce22115e","permalink":"https://shunk031.me/publication/kitada2019yans/","publishdate":"2019-08-27T00:00:00+09:00","relpermalink":"/publication/kitada2019yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 14 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2019. ** å¥¨åŠ±è³ ** å—è³","tags":["Domestic Conference","Non-refereed","Award","YANS","YANS2019"],"title":"è§£é‡ˆæ€§å‘ä¸Šã®ãŸã‚ã®æ³¨æ„æ©Ÿæ§‹ã¨æå¤±å‹¾é…ã«å¯¾ã™ã‚‹é–¢é€£æå¤±ã®å°å…¥","type":"publication"},{"authors":["Mahmoud Daif","Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Arabic"],"content":"","date":1566745200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"e7b89e123dd0a1a470c83f5d65cc2a18","permalink":"https://shunk031.me/publication/daif2019yans/","publishdate":"2019-08-26T00:00:00+09:00","relpermalink":"/publication/daif2019yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 14 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2019.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","YANS","YANS2019"],"title":"Image Based Character Embeddings for Arabic Document Classification","type":"publication"},{"authors":["é•·æ¾¤ é§¿å¤ª","åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Image-based Character Embedding"],"content":"","date":1566745200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"5edf76145097ee90381c1ddc83a3f24b","permalink":"https://shunk031.me/publication/nagasawa2019yans/","publishdate":"2019-08-26T00:00:00+09:00","relpermalink":"/publication/nagasawa2019yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 14 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2019.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","YANS","YANS2019"],"title":"æ—¥æœ¬èªã®æ–‡å­—ä½“ç³»ã‚’è€ƒæ…®ã—ãŸæ–‡æ›¸åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ææ¡ˆ","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Yoshifumi Seki"],"categories":["Interview"],"content":" ","date":1565276400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"97cfb3ba399df63f9cedad1944e50e6b","permalink":"https://shunk031.me/post/gunosy-intern-kdd-2019-owned-media/","publishdate":"2019-08-09T00:00:00+09:00","relpermalink":"/post/gunosy-intern-kdd-2019-owned-media/","section":"post","summary":"https://www.wantedly.com/companies/gunosy/post_articles/181818","tags":["Interview","Gunosy","Hosei University"],"title":"ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã‚‚æ‰€å±ï¼Gunosy ç ”ç©¶ãƒãƒ¼ãƒ ã®çµ„ç¹”ä½œã‚Šã¨åˆ¶åº¦ã‚’ã”ç´¹ä»‹","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Interviewed about my research internship in Gunosy Inc. ( the article).\nä»Šå¹´ã® 5 æœˆã€Gunosy ã§ã¯ã€Œåºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ä½œæˆæ”¯æ´ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨ Conditional Attention ã«ã‚ˆã‚‹ CV äºˆæ¸¬ã€ã®ç ”ç©¶çµæœã‚’ç™ºè¡¨ã—ãŸè«–æ–‡ãŒã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ã€ŒKDD2019ã€ã«æ¡æŠã•ã‚Œã¾ã—ãŸã€‚\nä»Šå›ã¯ã€ç ”ç©¶ã‚’é€²ã‚ãŸ Gunosy å…±åŒå‰µæ¥­è€…ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã«ãŠè©±ã‚’ä¼ºã£ã¦ãã¾ã—ãŸã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§ã‚‚ç ”ç©¶ã«æºã‚ã‚ŒãŸæŒ‡å°æ–¹æ³•ã¨ã¯ï¼Ÿæ¥­å‹™ã¨ä¸¦è¡Œã—ã¦è«–æ–‡ã‚’æ›¸ã‘ã‚‹è·å ´ç’°å¢ƒã¨ã¯ï¼Ÿã€ŒKDD2019ã€ã¸ã®ç™»å£‡ã‚’ç›´å‰ã«æ§ãˆãŸ 2 äººã®å¯¾è«‡ã‚’ãŠé€ã‚Šã—ã¾ã™ã€‚ãœã²ã€ã”è¦§ãã ã•ã„ã€‚\nSee also ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿã‚‚æ‰€å±ï¼Gunosy ç ”ç©¶ãƒãƒ¼ãƒ ã®çµ„ç¹”ä½œã‚Šã¨åˆ¶åº¦ã‚’ã”ç´¹ä»‹ ","date":1564671600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"a3c4c21d185eeb493ed0dc7ea0b61ab2","permalink":"https://shunk031.me/news/interview-about-my-research-internship-in-gunosy-2019/","publishdate":"2019-08-02T00:00:00+09:00","relpermalink":"/news/interview-about-my-research-internship-in-gunosy-2019/","section":"news","summary":"Interviewed about my research internship in Gunosy Inc. ( the article).\nä»Šå¹´ã® 5 æœˆã€Gunosy ã§ã¯ã€Œåºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ä½œæˆæ”¯æ´ã®ãŸã‚ã®ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨ Conditional Attention ã«ã‚ˆã‚‹ CV äºˆæ¸¬ã€ã®ç ”ç©¶çµæœã‚’ç™ºè¡¨ã—ãŸè«–æ–‡ãŒã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ã€ŒKDD2019ã€ã«æ¡æŠã•ã‚Œã¾ã—ãŸã€‚\n","tags":["News","Awards and Grants"],"title":"Interview About My Research Internship in Gunosy 2019","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Implementation"],"content":"","date":1564585200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"3462df1863821df2251107b7a7b69392","permalink":"https://shunk031.me/project/multi-task-conditional-attention-networks/","publishdate":"2019-08-01T00:00:00+09:00","relpermalink":"/project/multi-task-conditional-attention-networks/","section":"project","summary":"Official Implementation of the paper [Conversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creatives](https://arxiv.org/abs/1905.07289)","tags":["Chainer","Official Implementation"],"title":"Conversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creatives","type":"project"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got CEO special award in the second half of FY2019 at Gunosy Inc. (detail).\nCEO ç‰¹åˆ¥è³ï¼ˆå¤šå¤§ãªæ¥­ç¸¾ã«è²¢çŒ®ã—ãŸç¤¾å“¡ã®ã¿ã«è´ˆã‚‰ã‚Œã‚‹è³ï¼‰\nã“ã¡ã‚‰ã®è³ã¯å¾“æ¥ã®è³ã¨ã¯åˆ¥ã«ã€å¤šå¤§ãªæ¥­ç¸¾ã«è²¢çŒ®ã—ãŸç¤¾å“¡ãŒå±…ã‚‹å ´åˆã®ã¿è´ˆã‚‰ã‚Œã¾ã™ã€‚ä»Šå›ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹å›½éš›ä¼šè­°ã€ŒKDD2019ã€ã«è«–æ–‡ãŒæ¡æŠã•ã‚ŒãŸé–¢ã•ã‚“ã¨åŒ—ç”°ã•ã‚“ã® 2 åãŒå—è³ã—ã¾ã—ãŸã€‚\n","date":1564066800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"8b0aa03fda661a8408878818ac607e87","permalink":"https://shunk031.me/news/ceo-special-award-at-gunosy-2019/","publishdate":"2019-07-26T00:00:00+09:00","relpermalink":"/news/ceo-special-award-at-gunosy-2019/","section":"news","summary":"Got CEO special award in the second half of FY2019 at Gunosy Inc. (detail).\nCEO ç‰¹åˆ¥è³ï¼ˆå¤šå¤§ãªæ¥­ç¸¾ã«è²¢çŒ®ã—ãŸç¤¾å“¡ã®ã¿ã«è´ˆã‚‰ã‚Œã‚‹è³ï¼‰\n","tags":["News","Awards and Grants"],"title":"CEO special award in the second half of FY2019 at Gunosy Inc.","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Mitsumasa Kubo"],"categories":null,"content":" ","date":1563001500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"fd2a54e17750a9559446a3e41d71299e","permalink":"https://shunk031.me/event/ccse-2019/","publishdate":"2019-07-13T00:00:00+09:00","relpermalink":"/event/ccse-2019/","section":"event","summary":"On Predicting Ad Creative Evaluation using Deep Learning","tags":["Invited talk"],"title":"[Invited Talk] CCSE 2019","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":null,"content":" ","date":1559556000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"8388deec6cb29848938ca4c2805fded6","permalink":"https://shunk031.me/event/icml-kdd-2019-pre-conference-session/","publishdate":"2020-03-29T15:26:53+09:00","relpermalink":"/event/icml-kdd-2019-pre-conference-session/","section":"event","summary":" ","tags":["Invited Talk"],"title":"[Invited Talk] ICML/KDD 2019 Pre-conference session","type":"event"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Hosei University 100th year anniversary scholarship for master students (Detail : PDF ).\n","date":1559314800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"7e0db5b5a3a501867bbe3ab3c2a1c3bb","permalink":"https://shunk031.me/news/hosei-university-100th-year-anniversary-scholarship/","publishdate":"2019-06-01T00:00:00+09:00","relpermalink":"/news/hosei-university-100th-year-anniversary-scholarship/","section":"news","summary":"Got Hosei University 100th year anniversary scholarship for master students (Detail : PDF ).\n","tags":["News","Awards and Grants"],"title":"Hosei University 100th Year Anniversary Scholarship","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Press release"],"content":" æœ¬ç ”ç©¶ã¯å½“ç¤¾ã®å­¦ç”Ÿã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§ã‚ã‚‹åŒ—ç”° ä¿Šè¼” æ°ã¨ç ”ç©¶é–‹ç™ºãƒãƒ¼ãƒ ã®é–¢ å–œå²ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã€è«–æ–‡ã¯åŒ—ç”°æ°ã®æŒ‡å°æ•™å“¡ã§ã‚ã‚‹æ³•æ”¿å¤§å­¦ å½Œå†¨ ä» æ•™æˆã¨å…±ã«åŸ·ç­†ã•ã‚Œã¾ã—ãŸã€‚\n","date":1559055600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"47b54ece00213bc052777b8a864fa16d","permalink":"https://shunk031.me/post/gunosy-press-release-kdd-2019/","publishdate":"2019-05-29T00:00:00+09:00","relpermalink":"/post/gunosy-press-release-kdd-2019/","section":"post","summary":"https://gunosy.co.jp/news/172","tags":["Press release","Gunosy"],"title":"Gunosy ã®ç ”ç©¶è«–æ–‡ãŒãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã®å›½éš›ä¼šè­°ã€ŒKDD2019ã€ã«ã¦æ¡æŠ","type":"post"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi","Yoshifumi Seki"],"categories":["Computational Advertising"],"content":" ","date":1558882800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"48dd1990d1a1df4445b03a5fe6ed9f26","permalink":"https://shunk031.me/publication/kitada2019conversion/","publishdate":"2019-05-27T00:00:00+09:00","relpermalink":"/publication/kitada2019conversion/","section":"publication","summary":"Proc. of the 25th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining (KDD2019). (**Acceptance rate = 20%**; [1st place in Data Mining \u0026 Analysis at Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues\u0026hl=en\u0026vq=eng_datamininganalysis))","tags":["International Conference","Refereed","Computational Advertising","Gunosy","International Publication","KDD","KDD2019"],"title":"Conversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creative","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi","Yoshifumi Seki"],"categories":["News"],"content":"The following paper has been accepted to the KDD2019 Applied Data Science Track.\nShunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. â€œConversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creativeâ€ ","date":1556463600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"b8525184fb1dd06761b86d2fd7e9d7d0","permalink":"https://shunk031.me/news/acceptance-to-kdd2019/","publishdate":"2019-04-29T00:00:00+09:00","relpermalink":"/news/acceptance-to-kdd2019/","section":"news","summary":"The following paper has been accepted to the KDD2019 Applied Data Science Track.\nShunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. â€œConversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creativeâ€ ","tags":["News"],"title":"Accepted our paper to ACM KDD2019 Applied Data Science Track","type":"news"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":[],"content":" ","date":1554908400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"2b656b9895181e9443fb5c92b2933d90","permalink":"https://shunk031.me/post/award-ipsj-2019/","publishdate":"2019-04-11T00:00:00+09:00","relpermalink":"/post/award-ipsj-2019/","section":"post","summary":"https://www.hosei.ac.jp/gs/NEWS/topics/jusho/190411_4","tags":["Award","Hosei University"],"title":"ç†å·¥å­¦ç ”ç©¶ç§‘åœ¨å­¦ç”Ÿã®åŒ—ç”°ä¿Šè¼”æ°ãŒã€Œå­¦ç”Ÿå¥¨åŠ±è³ã€ã‚’å—è³","type":"post"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got Student Honorable Mention in IPSJ 2019.\nSee also ç†å·¥å­¦ç ”ç©¶ç§‘åœ¨å­¦ç”Ÿã®åŒ—ç”°ä¿Šè¼”æ°ãŒã€Œå­¦ç”Ÿå¥¨åŠ±è³ã€ã‚’å—è³ ","date":1552489200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"15f65c901314241263aed6f9c238a0c9","permalink":"https://shunk031.me/news/student-honorable-mention-ipsj2019/","publishdate":"2019-03-14T00:00:00+09:00","relpermalink":"/news/student-honorable-mention-ipsj2019/","section":"news","summary":"é ‘å¥ãªçš®è†šè…«ç˜è¨ºæ–­æ”¯æ´ã®ãŸã‚ã® body hair augmentation","tags":["News","Awards and Grants"],"title":"Student Honorable Mention IPSJ 2019","type":"news"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Medical Image Processing","Skin Lesion Diagnosis"],"content":"","date":1552489200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ece07baa0a8f4645d3337ee30a5c7798","permalink":"https://shunk031.me/publication/kitada2019ipsj/","publishdate":"2019-03-14T00:00:00+09:00","relpermalink":"/publication/kitada2019ipsj/","section":"publication","summary":"æƒ…å ±å‡¦ç†å­¦ç¬¬ 81 å›å…¨å›½å¤§ä¼šï¼Œ2019.","tags":["Domestic Conference","Non-refereed","Medical Image Processing","IPSJ","IPSJ2019"],"title":"é ‘å¥ãªçš®è†šè…«ç˜è¨ºæ–­æ”¯æ´ã®ãŸã‚ã® body hair augmentation","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»","é–¢ å–œå²"],"categories":["Computational Advertising","Natural Language Processing"],"content":"","date":1552402800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"513fd31a3c859f9512775e5167f603c4","permalink":"https://shunk031.me/publication/kitada2019nlp/","publishdate":"2019-03-13T00:00:00+09:00","relpermalink":"/publication/kitada2019nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 25 å›å¹´æ¬¡å¤§ä¼šï¼Œ2019.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","Computational Advertising","ANLP","Gunosy","NLP2019"],"title":"åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–è‡ªå‹•ç”Ÿæˆã«ã‚€ã‘ãŸãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã¨ Conditional Attention ã«ã‚ˆã‚‹ CVR äºˆæ¸¬","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got overall 1st prize and medium 2nd prize in DeNA Data Analysis Competition Event.\n","date":1548514800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"6d216e69973852d53f4bb474fe2b35ff","permalink":"https://shunk031.me/news/first-prize-of-dena-data-analysis-competition-event-2019/","publishdate":"2019-01-27T00:00:00+09:00","relpermalink":"/news/first-prize-of-dena-data-analysis-competition-event-2019/","section":"news","summary":"Got overall 1st prize and medium 2nd prize in DeNA Data Analysis Competition Event.\n","tags":["News","Awards and Grants"],"title":"Overall 1st prize and medium 2nd prize in DeNA Data Analysis Competition Event","type":"news"},{"authors":["Shunsuke Kitada, Ph.D.","Ryunosuke Kotani","Hitoshi Iyatomi"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":" ","date":1538924400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"64a3584414767a836a33a67f85f0f974","permalink":"https://shunk031.me/publication/kitada2018end/","publishdate":"2020-03-29T02:56:29+09:00","relpermalink":"/publication/kitada2018end/","section":"publication","summary":"Proc. of IEEE Applied Imagery Pattern Recognition (AIPR) 2018 Workshop","tags":["International Conference","Refereed","Natural Language Processing","International Publication","AIPR","AIPR2018"],"title":"End-to-End Text Classification via Image-based Embedding using Character-level Networks","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Hitoshi Iyatomi"],"categories":["Medical Image Processing","Skin Lesion Diagnosis"],"content":" ","date":1536246000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"495c03a39d0cc4074b39e95dc833cb84","permalink":"https://shunk031.me/publication/kitada2018skin/","publishdate":"2020-03-29T03:03:35+09:00","relpermalink":"/publication/kitada2018skin/","section":"publication","summary":"Manuscript for [the International Skin Imaging Collaboration 2018](https://challenge2018.isic-archive.com/).","tags":["Preprint","Medical Image Processing"],"title":"Skin lesion classification with ensemble of squeeze-and-excitation networks and semi-supervised learning","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D.","Ryunosuke Kotani","Hitoshi Iyatomi"],"categories":["News"],"content":"The following paper has been accepted to the AIPR2018 Workshop.\nShunsuke Kitada, Ryunosuke Kotani, and Hitoshi Iyatomi. â€œEnd-to-End Text Classification via Image-based Embedding using Character-level Networksâ€ ","date":1536073200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"54bd4b45ce9f20067555b1d047636343","permalink":"https://shunk031.me/news/acceptance-to-ieee-aipr2018/","publishdate":"2018-09-05T00:00:00+09:00","relpermalink":"/news/acceptance-to-ieee-aipr2018/","section":"news","summary":"The following paper has been accepted to the AIPR2018 Workshop.\nShunsuke Kitada, Ryunosuke Kotani, and Hitoshi Iyatomi. â€œEnd-to-End Text Classification via Image-based Embedding using Character-level Networksâ€ ","tags":["News"],"title":"Accepted our paper to IEEE AIPR2018 Workshop","type":"news"},{"authors":["åŒ—ç”° ä¿Šè¼”","é–¢ å–œå²","å½Œå†¨ ä»"],"categories":["Computational Advertising","Natural Language Processing"],"content":"","date":1535382000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"59f8900a894041df205133fc524f0cc7","permalink":"https://shunk031.me/publication/kitada2018yans/","publishdate":"2018-08-28T00:00:00+09:00","relpermalink":"/publication/kitada2018yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 13 å›ã‚·ãƒ³ãƒã‚¸ã‚¦ãƒ ï¼Œ2018.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","Computational Advertising","Gunosy","YANS","YANS2018"],"title":"åºƒå‘Šã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–è‡ªå‹•ç”Ÿæˆã«å‘ã‘ãŸå˜èªãƒ¬ãƒ™ãƒ«ã§ã®è©•ä¾¡æ‰‹æ³•ã®æ¤œè¨","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["Award"],"content":" ä½œå“å Chash Box ãƒãƒ¼ãƒ å Cpaw MobileNet ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ã‚‚ã¨ã«ã—ãŸ Siamese Network ã«ã‚ˆã‚‹é¡”èªè¨¼ã¨éŸ³å£°èªè­˜ã‚’ç”¨ã„ãŸ AI ã‚¹ãƒãƒ¼ãƒˆãƒœãƒƒã‚¯ã‚¹ ","date":1533448800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"ac2bca08e85d9fb7d8f4682eb13de89f","permalink":"https://shunk031.me/post/hacku-hosei-2018/","publishdate":"2018-08-05T15:00:00+09:00","relpermalink":"/post/hacku-hosei-2018/","section":"post","summary":"https://hacku.yahoo.co.jp/hosei2018/","tags":["Award","Youtube"],"title":"HackU æ³•æ”¿ 2018 æœ€å„ªç§€è³ å—è³","type":"post"},{"authors":["Cpaw"],"categories":["News","Awards and Grants"],"content":"Got 1st prize of hackU Hosei 2018.\nSee also â€œHackU æ³•æ”¿ 2018 æœ€å„ªç§€è³å—è³â€ ","date":1533049200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"f882681f6df053ef1767a24c798c6a66","permalink":"https://shunk031.me/news/first-prize-of-hacku-hosei-2018/","publishdate":"2018-08-01T00:00:00+09:00","relpermalink":"/news/first-prize-of-hacku-hosei-2018/","section":"news","summary":"Got 1st prize of hackU Hosei 2018.\nSee also â€œHackU æ³•æ”¿ 2018 æœ€å„ªç§€è³å—è³â€ ","tags":["News","Awards and Grants"],"title":"First Prize of Hacku Hosei 2018","type":"news"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":"","date":1521039600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"0db84bc16cde69cfbd622558d5ba5f5c","permalink":"https://shunk031.me/publication/kitada2018nlp/","publishdate":"2018-03-15T00:00:00+09:00","relpermalink":"/publication/kitada2018nlp/","section":"publication","summary":"è¨€èªå‡¦ç†å­¦ä¼šç¬¬ 24 å›å¹´æ¬¡å¤§ä¼šï¼Œ2018.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","ANLP","NLP2018"],"title":"CE-CLCNN: Character Encoder ã‚’ç”¨ã„ãŸ Character-level Convolutional Neural Networks ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡","type":"publication"},{"authors":["åŒ—ç”° ä¿Šè¼”","å½Œå†¨ ä»"],"categories":["Natural Language Processing","Glyph-aware NLP","NLP for Asian Languages"],"content":"","date":1504537200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"9fed0119658279e0589f50504b658706","permalink":"https://shunk031.me/publication/kitada2017yans/","publishdate":"2017-09-05T00:00:00+09:00","relpermalink":"/publication/kitada2017yans/","section":"publication","summary":"NLP è‹¥æ‰‹ã®ä¼š (YANS) ç¬¬ 12 å›ã‚·ãƒ³ãƒ›ã‚šã‚·ã‚™ã‚¦ãƒ ï¼Œ2017.","tags":["Domestic Conference","Non-refereed","Natural Language Processing","YANS","YANS2017"],"title":"Character-level Convolutional Neural Networks ã«ãŠã‘ã‚‹ Wildcard Training ã®åŸºç¤æ¤œè¨","type":"publication"},{"authors":["Shunsuke Kitada, Ph.D."],"categories":["News","Awards and Grants"],"content":"Got student award at FR FRONTIER.\n","date":1501513200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770812977,"objectID":"f68fbd048a045c43f2bdea0809825651","permalink":"https://shunk031.me/news/student-award-at-fr-frontier/","publishdate":"2017-08-01T00:00:00+09:00","relpermalink":"/news/student-award-at-fr-frontier/","section":"news","summary":"ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ç”»åƒã«ãŠã‘ã‚‹æ´‹æœã®ã€Œè‰²ã€åˆ†é¡","tags":["News","Awards and Grants"],"title":"Got Student Award at FR FRONTIER","type":"news"}]