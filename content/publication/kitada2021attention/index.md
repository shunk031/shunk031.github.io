---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training"
authors: ["Shunsuke Kitada", "Hitoshi Iyatomi"]
date: 2021-06-29T00:00:00+00:00
doi: "10.1109/ACCESS.2021.3093456"

# Schedule page publish date (NOT publication's date).
publishDate: 2021-06-29T00:00:00+00:00

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["2"]

# Publication name and optional abbreviated publication name.
publication: "IEEE Access"
publication_short: "IEEE Access"

abstract: "Although attention mechanisms have been applied to a variety of deep learning models and have been shown to improve the prediction performance, it has been reported to be vulnerable to perturbations to the mechanism. To overcome the vulnerability to perturbations in the mechanism, we are inspired by adversarial training (AT), which is a powerful regularization technique for enhancing the robustness of the models. In this paper, we propose a general training technique for natural language processing tasks, including AT for attention (Attention AT) and more interpretable AT for attention (Attention iAT). The proposed techniques improved the prediction performance and the model interpretability by exploiting the mechanisms with AT. In particular, Attention iAT boosts those advantages by introducing adversarial perturbation, which enhances the difference in the attention of the sentences. Evaluation experiments with ten open datasets revealed that AT for attention mechanisms, especially Attention iAT, demonstrated (1) the best performance in nine out of ten tasks and (2) more interpretable attention (i.e., the resulting attention correlated more strongly with gradient-based word importance) for all tasks. Additionally, the proposed techniques are (3) much less dependent on perturbation size in AT."

# Summary. An optional shortened abstract.
summary: "IEEE Access (**Impact Factor: 3.367** in 2020; [1st place in Engineering & Computer Science (general) at Google Scholar Metrics](https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_enggeneral))"

tags: ["Journal", "International Publication", "Natural Language Processing", "Referred", "Open Access", "IEEE"]
categories: ["Natural Language Processing", "Attention Mechanisms", "Adversarial Training", "Text Classification", "Question Answering", "Natural Language Inference"]
featured: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
links:
- name: Preprint
  url: https://arxiv.org/abs/2009.12064
  icon_pack: ai
  icon: arxiv
- name: Code
  url: https://github.com/shunk031/attention-meets-perturbation
  icon_pack: fab
  icon: github

url_pdf: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9947020
url_code:
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: true

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

| arXiv | IEEE Xplore | SCImago |
|-------|-------------|---------|
| {{< blogcard url="https://arxiv.org/abs/2009.12064" >}} | {{< blogcard url="https://ieeexplore.ieee.org/document/9467291" >}} | <a href="https://www.scimagojr.com/journalsearch.php?q=21100374601&amp;tip=sid&amp;exact=no" title="SCImago Journal &amp; Country Rank"><img border="0" src="https://www.scimagojr.com/journal_img.php?id=21100374601" alt="SCImago Journal &amp; Country Rank"  /></a> |
